{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Z0tBzrtE7t0483XKjbXnsom2v4XRWd5T",
      "authorship_tag": "ABX9TyOjHtLeq/dQUVeiFvOQczIQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyuwonh-ai/SeSAC_miniProject_1Team/blob/main/hundred/Hundred_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hundred 자세의 기준을 잡기위한 EDA"
      ],
      "metadata": {
        "id": "wqXHf6lEyIJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "tQ-7agVequS_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80469c9c-15a2-4a0a-e2ac-9ea8ae359959"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from scipy.stats import linregress\n",
        "\n",
        "\n",
        "# --- 2. 분석 함수 정의 ---\n",
        "def calculate_hundred_metrics(df):\n",
        "    \"\"\"\n",
        "    Hundred 지표 (각도, 높이)를 계산\n",
        "    - (Smart Leg) 신뢰도 높은 다리 자동 선택\n",
        "    - (abs(dx)) 방향에 상관없이 예각(Angle) 계산\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # (1) 다리 선택 (잘 보이는 쪽으로)\n",
        "    # 원본 _conf 컬럼이 있는지 확인\n",
        "    if 'Left_Ankle_conf' not in df.columns or 'Right_Ankle_conf' not in df.columns:\n",
        "        print(\"    ⚠️ Ankle_conf 컬럼이 없어 '왼쪽 다리'를 기준으로 계산합니다.\")\n",
        "        use_left_leg_mask = True # conf 없으면 왼쪽 기준\n",
        "    else:\n",
        "        use_left_leg_mask = df['Left_Ankle_conf'] >= df['Right_Ankle_conf']\n",
        "\n",
        "    smart_ankle_x = np.where(use_left_leg_mask, df['norm_Left_Ankle_x'], df['norm_Right_Ankle_x'])\n",
        "    smart_ankle_y = np.where(use_left_leg_mask, df['norm_Left_Ankle_y'], df['norm_Right_Ankle_y'])\n",
        "    smart_hip_x = np.where(use_left_leg_mask, df['norm_Left_Hip_x'], df['norm_Right_Hip_x'])\n",
        "    smart_hip_y = np.where(use_left_leg_mask, df['norm_Left_Hip_y'], df['norm_Right_Hip_y'])\n",
        "\n",
        "    # (1_A) 다리 높이 각도 (Smart Leg 기준)\n",
        "    dy = -(smart_ankle_y - smart_hip_y) # Y축 반전\n",
        "    dx = np.abs(smart_ankle_x - smart_hip_x) # [수정] 절댓값 적용\n",
        "    df_out['Leg_Height_Angle'] = np.degrees(np.arctan2(dy, dx))\n",
        "\n",
        "    # (2) 코어 고정각 (Shoulder-Hip-Knee)\n",
        "    def get_angle(a, b, c):\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
        "\n",
        "    p_shoulder = df[['norm_Left_Shoulder_x', 'norm_Left_Shoulder_y']].values\n",
        "    p_hip = df[['norm_Left_Hip_x', 'norm_Left_Hip_y']].values\n",
        "    p_knee = df[['norm_Left_Knee_x', 'norm_Left_Knee_y']].values\n",
        "\n",
        "    df_out['Core_Angle'] = [get_angle(s, h, k) if not (np.isnan(s).any() or np.isnan(h).any() or np.isnan(k).any()) else np.nan\n",
        "                            for s, h, k in zip(p_shoulder, p_hip, p_knee)]\n",
        "\n",
        "    # (3) 상체 컬업 (Nose가 Shoulder Center보다 얼마나 위에 있나)\n",
        "    shoulder_center_y = (df['norm_Left_Shoulder_y'] + df['norm_Right_Shoulder_y']) / 2\n",
        "    df_out['Head_Curl_Position'] = shoulder_center_y - df['norm_Nose_y']\n",
        "\n",
        "    return df_out\n",
        "\n",
        "def analyze_stability(group):\n",
        "    \"\"\"\n",
        "    하나의 영상(group)을 받아 안정성 지표를 계산\n",
        "    [업그레이드!] 자세의 \"가장 긴 연속 구간(Longest Streak)\"을 'Active'로 감지\n",
        "    \"\"\"\n",
        "\n",
        "    # --- [보간(Interpolation) 전처리] ---\n",
        "    # (이 부분은 이전과 동일)\n",
        "    parts_to_clean = ['Left_Ankle', 'Right_Ankle', 'Left_Hip', 'Right_Hip',\n",
        "                      'Left_Shoulder', 'Right_Shoulder', 'Nose', 'Left_Knee']\n",
        "    group_clean = group.copy()\n",
        "    for part in parts_to_clean:\n",
        "        if f'{part}_conf' in group_clean.columns:\n",
        "            low_conf_mask = group_clean[f'{part}_conf'] < 0.5\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_x'] = np.nan\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_y'] = np.nan\n",
        "    norm_cols = [col for col in group_clean.columns if col.startswith('norm_')]\n",
        "    group_clean[norm_cols] = group_clean[norm_cols].interpolate(method='linear', limit_direction='both', limit=5)\n",
        "    # --- [전처리 끝] ---\n",
        "\n",
        "    # 1. 'Hundred' 지표 계산\n",
        "    group_with_metrics = calculate_hundred_metrics(group_clean) # (이 함수는 이전에 정의됨)\n",
        "    group_with_metrics = group_with_metrics.dropna(subset=['Leg_Height_Angle', 'Head_Curl_Position'])\n",
        "\n",
        "    if group_with_metrics.empty:\n",
        "        return None\n",
        "\n",
        "    # --- [핵심 수정] Longest Streak 로직 ---\n",
        "    # (1) \"후보 프레임\" 찾기 (물리적 조건)\n",
        "    group_with_metrics['is_candidate'] = (\n",
        "        (group_with_metrics['Leg_Height_Angle'] > 5) &\n",
        "        (group_with_metrics['Leg_Height_Angle'] < 45) &\n",
        "        (group_with_metrics['Head_Curl_Position'] > 0.05)\n",
        "    )\n",
        "\n",
        "    # (2) 연속된 \"True\" 블록(Streak)에 ID 부여하기\n",
        "    # (is_candidate 값이 바뀔 때마다 cumsum()으로 새 ID를 만듦)\n",
        "    group_with_metrics['block_id'] = (group_with_metrics['is_candidate'] != group_with_metrics['is_candidate'].shift()).cumsum()\n",
        "\n",
        "    # (3) \"True\" 블록(후보 구간)들의 길이(프레임 수) 계산\n",
        "    candidate_streaks = group_with_metrics[group_with_metrics['is_candidate'] == True].groupby('block_id').size()\n",
        "\n",
        "    if candidate_streaks.empty:\n",
        "        # 'True'인 블록이 하나도 없음 (운동 안 함)\n",
        "        return None\n",
        "\n",
        "    # (4) \"가장 긴\" 블록(Streak)의 ID 찾기\n",
        "    longest_block_id = candidate_streaks.idxmax()\n",
        "\n",
        "    # (5) \"가장 긴\" 블록의 프레임들만 'Active'로 정의\n",
        "    active_frames = group_with_metrics[\n",
        "        (group_with_metrics['block_id'] == longest_block_id) &\n",
        "        (group_with_metrics['is_candidate'] == True)\n",
        "    ].copy()\n",
        "    # --- [수정 완료] ---\n",
        "\n",
        "    if len(active_frames) < 10: # 가장 긴 구간도 10프레임 미만이면 무시\n",
        "        return None\n",
        "\n",
        "    # (2) 지표 계산: 흔들림 (표준편차)\n",
        "    stability_score = active_frames['Leg_Height_Angle'].std()\n",
        "    core_stability = active_frames['Core_Angle'].std()\n",
        "\n",
        "    # (3) 지표 계산: 다리 처짐 (기울기)\n",
        "    if len(active_frames['Frame'].unique()) > 1:\n",
        "        slope, _ = np.polyfit(active_frames['Frame'], active_frames['Leg_Height_Angle'], 1)\n",
        "    else:\n",
        "        slope = 0\n",
        "\n",
        "    start_frame = active_frames['Frame'].min()\n",
        "    end_frame = active_frames['Frame'].max()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Leg_Stability_StdDev': stability_score,\n",
        "        'Core_Stability_StdDev': core_stability,\n",
        "        'Leg_Fatigue_Slope': slope,\n",
        "        'Avg_Leg_Angle': active_frames['Leg_Height_Angle'].mean(),\n",
        "        'Avg_Core_Angle': active_frames['Core_Angle'].mean(),\n",
        "        'Active_Frame_Count': len(active_frames),\n",
        "        'Active_Start_Frame': start_frame,\n",
        "        'Active_End_Frame': end_frame\n",
        "    })\n",
        "\n",
        "# --- 3. 메인 실행부 (Loop 방식) ---\n",
        "\n",
        "# [경로 설정]\n",
        "drive_root = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/dataset/Hundred'\n",
        "norm_folder_path = os.path.join(drive_root, 'keypoint_Hundred', 'norm')\n",
        "all_files = glob.glob(os.path.join(norm_folder_path, \"norm_*.csv\"))\n",
        "\n",
        "output_path = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis'\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"❌ '{norm_folder_path}' 경로에 'norm_*.csv' 파일이 없습니다. 경로를 확인해주세요.\")\n",
        "else:\n",
        "    print(f\"✅ 총 {len(all_files)}개의 정규화된 CSV 파일을 분석합니다.\")\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for i, file_path in enumerate(all_files):\n",
        "        filename = os.path.basename(file_path)\n",
        "        print(f\"  [{i+1}/{len(all_files)}] 분석 중: {filename}\")\n",
        "\n",
        "        try:\n",
        "            # 1. 파일명에서 메타데이터(ID, Level) 추출 (안전한 방식으로 수정)\n",
        "            clean_name = filename.replace('norm_', '').replace('.csv', '')\n",
        "            parts = clean_name.split('_')\n",
        "\n",
        "            place = parts[2]\n",
        "            level = parts[5]\n",
        "            person_id = parts[6]\n",
        "            pose_name = parts[4]\n",
        "\n",
        "            # 키워드로 검색 (순서가 바뀌어도 OK)\n",
        "            for part in parts:\n",
        "                if part.startswith('actor'):\n",
        "                    person_id = part\n",
        "                if part in ['고급', '중급', '초급']:\n",
        "                    level = part\n",
        "                if part.lower() == 'hundred':\n",
        "                    pose_name = 'Hundred'\n",
        "\n",
        "            # 'Hundred' 파일이 아니면 건너뛰기\n",
        "            if pose_name != 'Hundred':\n",
        "                print(f\"    ... [Skip] 'Hundred' 파일이 아니므로 건너뜁니다.\")\n",
        "                continue\n",
        "\n",
        "            # 2. CSV 읽기\n",
        "            df_single_file = pd.read_csv(file_path)\n",
        "\n",
        "            # 3. '안정성' 분석 (보간 + 지표 계산)\n",
        "            # (calculate_hundred_metrics는 analyze_stability 내부에서 호출됨)\n",
        "            stability_summary = analyze_stability(df_single_file)\n",
        "\n",
        "            # 4. 결과 저장\n",
        "            if stability_summary is not None:\n",
        "                stability_summary['Source_File'] = filename\n",
        "                stability_summary['Person_ID'] = person_id\n",
        "                stability_summary['Level'] = level\n",
        "                stability_summary['Place'] = place\n",
        "                results_list.append(stability_summary)\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"    ⚠️ {filename} 처리 중 오류 발생 (KeyError): {e} - CSV에 필요한 컬럼이 없습니다.\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ {filename} 처리 중 알 수 없는 오류 발생: {e}\")\n",
        "\n",
        "    # --- 4. 최종 결과 저장 (오류 수정된 버전) ---\n",
        "    if results_list:\n",
        "        df_final_eda = pd.DataFrame(results_list)\n",
        "\n",
        "        output_dir = os.path.join(output_path, \"keypoint_Hundred_Analysis_vFinal\") # 새 폴더 이름\n",
        "        save_path = os.path.join(output_dir, \"EDA_Hundred_Stability_Report_vFinal_Longest_Streak.csv\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        df_final_eda.to_csv(save_path, index=False)\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\"✅ 'Hundred' 안정성 분석 완료! (모든 기능 통합)\")\n",
        "        print(f\"저장된 파일: {save_path}\")\n",
        "        print(f\"총 {len(df_final_eda)}개의 'Hundred' 영상이 분석되었습니다.\")\n",
        "\n",
        "        print(\"\\n[결과 미리보기]\")\n",
        "        print(df_final_eda.head())\n",
        "    else:\n",
        "        print(\"분석할 'Hundred' 영상이 없거나 유효한 데이터가 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kyJkouzUuB-",
        "outputId": "a94313a0-7fb1-4a97-c4ba-bfde019a154b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 총 117개의 정규화된 CSV 파일을 분석합니다.\n",
            "  [1/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.51.54_CAM_1.csv\n",
            "  [2/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.16.15_CAM_1.csv\n",
            "  [3/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.14.40_CAM_1.csv\n",
            "  [4/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.15.35_CAM_1.csv\n",
            "  [5/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.17.02_CAM_1.csv\n",
            "  [6/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP076_20220929_15.26.04_CAM_1.csv\n",
            "  [7/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.10.42_CAM_1.csv\n",
            "  [8/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.11.30_CAM_1.csv\n",
            "  [9/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.12.19_CAM_1.csv\n",
            "  [10/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.01.17_CAM_1.csv\n",
            "  [11/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.02.15_CAM_1.csv\n",
            "  [12/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.03.10_CAM_1.csv\n",
            "  [13/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.50.10_CAM_1.csv\n",
            "  [14/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.51.01_CAM_1.csv\n",
            "  [15/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.52.49_CAM_1.csv\n",
            "  [16/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.53.42_CAM_1.csv\n",
            "  [17/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.35.27_CAM_1.csv\n",
            "  [18/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.36.16_CAM_1.csv\n",
            "  [19/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.37.05_CAM_1.csv\n",
            "  [20/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.37.52_CAM_1.csv\n",
            "  [21/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.38.46_CAM_1.csv\n",
            "  [22/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.39.39_CAM_1.csv\n",
            "  [23/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP092_20221005_11.17.31_CAM_1.csv\n",
            "  [24/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.36.09_CAM_1.csv\n",
            "  [25/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.37.03_CAM_1.csv\n",
            "  [26/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.37.57_CAM_1.csv\n",
            "  [27/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.38.54_CAM_1.csv\n",
            "  [28/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.39.44_CAM_1.csv\n",
            "  [29/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.40.59_CAM_1.csv\n",
            "  [30/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.42.02_CAM_1.csv\n",
            "  [31/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.42.56_CAM_1.csv\n",
            "  [32/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.43.45_CAM_1.csv\n",
            "  [33/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.16.59_CAM_1.csv\n",
            "  [34/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.17.46_CAM_1.csv\n",
            "  [35/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.18.37_CAM_1.csv\n",
            "  [36/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.19.24_CAM_1.csv\n",
            "  [37/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.20.17_CAM_1.csv\n",
            "  [38/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.22.53_CAM_1.csv\n",
            "  [39/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.26.00_CAM_1.csv\n",
            "  [40/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.13.16_CAM_1.csv\n",
            "  [41/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.14.22_CAM_1.csv\n",
            "  [42/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.15.20_CAM_1.csv\n",
            "  [43/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.16.13_CAM_1.csv\n",
            "  [44/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.17.17_CAM_1.csv\n",
            "  [45/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.18.38_CAM_1.csv\n",
            "  [46/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.19.28_CAM_1.csv\n",
            "  [47/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.01.06_CAM_1.csv\n",
            "  [48/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.01.55_CAM_1.csv\n",
            "  [49/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.02.45_CAM_1.csv\n",
            "  [50/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.03.39_CAM_1.csv\n",
            "  [51/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.04.44_CAM_1.csv\n",
            "  [52/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.20.51_CAM_1.csv\n",
            "  [53/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.21.48_CAM_1.csv\n",
            "  [54/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.15.09_CAM_1.csv\n",
            "  [55/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.17.09_CAM_1.csv\n",
            "  [56/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.18.05_CAM_1.csv\n",
            "  [57/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.05.59_CAM_1.csv\n",
            "  [58/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.36.07_CAM_1.csv\n",
            "  [59/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.36.53_CAM_1.csv\n",
            "  [60/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.38.28_CAM_1.csv\n",
            "  [61/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.37.42_CAM_1.csv\n",
            "  [62/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.40.12_CAM_1.csv\n",
            "  [63/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.39.18_CAM_1.csv\n",
            "  [64/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.42.35_CAM_1.csv\n",
            "  [65/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.41.49_CAM_1.csv\n",
            "  [66/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.40.59_CAM_1.csv\n",
            "  [67/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.43.23_CAM_1.csv\n",
            "  [68/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_15.11.39_CAM_1.csv\n",
            "  [69/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.04.49_CAM_1.csv\n",
            "  [70/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.07.10_CAM_1.csv\n",
            "  [71/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.10.55_CAM_1.csv\n",
            "  [72/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.08.25_CAM_1.csv\n",
            "  [73/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.13.11_CAM_1.csv\n",
            "  [74/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.12.05_CAM_1.csv\n",
            "  [75/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.40.24_CAM_1.csv\n",
            "  [76/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.41.26_CAM_1.csv\n",
            "  [77/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.42.27_CAM_1.csv\n",
            "  [78/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.44.29_CAM_1.csv\n",
            "  [79/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.43.26_CAM_1.csv\n",
            "  [80/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.45.20_CAM_1.csv\n",
            "  [81/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.46.08_CAM_1.csv\n",
            "  [82/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.18.00_CAM_1.csv\n",
            "  [83/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.16.51_CAM_1.csv\n",
            "  [84/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.15.49_CAM_1.csv\n",
            "  [85/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.19.49_CAM_1.csv\n",
            "  [86/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.19.05_CAM_1.csv\n",
            "  [87/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.21.38_CAM_1.csv\n",
            "  [88/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.20.45_CAM_1.csv\n",
            "  [89/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.08.20_CAM_1.csv\n",
            "  [90/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.07.30_CAM_1.csv\n",
            "  [91/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.09.19_CAM_1.csv\n",
            "  [92/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.10.25_CAM_1.csv\n",
            "  [93/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.12.07_CAM_1.csv\n",
            "  [94/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.11.17_CAM_1.csv\n",
            "  [95/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.12.55_CAM_1.csv\n",
            "  [96/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.13.47_CAM_1.csv\n",
            "  [97/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.30.00_CAM_1.csv\n",
            "  [98/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.30.46_CAM_1.csv\n",
            "  [99/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.08.04_CAM_1.csv\n",
            "  [100/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.07.14_CAM_1.csv\n",
            "  [101/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.31.33_CAM_1.csv\n",
            "  [102/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.08.53_CAM_1.csv\n",
            "  [103/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.09.55_CAM_1.csv\n",
            "  [104/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.10.45_CAM_1.csv\n",
            "  [105/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.11.37_CAM_1.csv\n",
            "  [106/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.12.29_CAM_1.csv\n",
            "  [107/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.13.19_CAM_1.csv\n",
            "  [108/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.22.43_CAM_1.csv\n",
            "  [109/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.23.38_CAM_1.csv\n",
            "  [110/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.24.32_CAM_1.csv\n",
            "  [111/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.25.26_CAM_1.csv\n",
            "  [112/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.27.16_CAM_1.csv\n",
            "  [113/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.26.20_CAM_1.csv\n",
            "  [114/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.52.51_CAM_1.csv\n",
            "  [115/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.54.00_CAM_1.csv\n",
            "  [116/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.54.55_CAM_1.csv\n",
            "  [117/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.55.52_CAM_1.csv\n",
            "--------------------------------------------------\n",
            "✅ 'Hundred' 안정성 분석 완료! (모든 기능 통합)\n",
            "저장된 파일: /content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis/keypoint_Hundred_Analysis_vFinal/EDA_Hundred_Stability_Report_vFinal_Longest_Streak.csv\n",
            "총 97개의 'Hundred' 영상이 분석되었습니다.\n",
            "\n",
            "[결과 미리보기]\n",
            "   Leg_Stability_StdDev  Core_Stability_StdDev  Leg_Fatigue_Slope  \\\n",
            "0              4.299031               5.196202           0.013573   \n",
            "1              1.783692               5.600530          -0.028082   \n",
            "2              1.614619               6.713806           0.003698   \n",
            "3              3.133009               6.363409          -0.007825   \n",
            "4              2.657241               6.461343          -0.019675   \n",
            "\n",
            "   Avg_Leg_Angle  Avg_Core_Angle  Active_Frame_Count  Active_Start_Frame  \\\n",
            "0      20.775256      149.495737               140.0               261.0   \n",
            "1      10.040336      158.375982               128.0               284.0   \n",
            "2      10.120511      159.101067               150.0               226.0   \n",
            "3      18.117616      147.638378               161.0               240.0   \n",
            "4      15.864876      149.807883               138.0               241.0   \n",
            "\n",
            "   Active_End_Frame                                        Source_File  \\\n",
            "0             400.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "1             411.0  norm_필라테스_가산_C_Mat_Hundred_고급_actor...   \n",
            "2             375.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "3             400.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "4             378.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "\n",
            "   Person_ID  Level Place  \n",
            "0  actorP082  고급     A  \n",
            "1  actorP079  고급     C  \n",
            "2  actorP075  고급     A  \n",
            "3  actorP075  고급     A  \n",
            "4  actorP075  고급     A  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from scipy.stats import linregress\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. 드라이브 마운트 (최초 1회 실행)\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    pass # 이미 마운트됨\n",
        "\n",
        "# --- [신규 함수] 각도 계산 헬퍼 함수 ---\n",
        "def get_relative_lift_angle(neck, hip, ankle):\n",
        "    \"\"\"\n",
        "    Hip을 중심점으로, Neck-Hip 벡터와 Hip-Ankle 벡터 사이의 각도를 계산합니다.\n",
        "    결과는 180 - 사이각으로, 바닥(몸통 연장선)에서 들어올린 각도를 나타냅니다.\n",
        "    \"\"\"\n",
        "    vec_torso = neck - hip\n",
        "    vec_leg = ankle - hip\n",
        "\n",
        "    dot_product = np.dot(vec_torso, vec_leg)\n",
        "    norm_torso = np.linalg.norm(vec_torso)\n",
        "    norm_leg = np.linalg.norm(vec_leg)\n",
        "\n",
        "    if norm_torso == 0 or norm_leg == 0:\n",
        "        return 0\n",
        "\n",
        "    # 코사인 법칙 계산 및 클리핑\n",
        "    cosine_angle = dot_product / (norm_torso * norm_leg)\n",
        "    angle_rad = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
        "    angle_deg = np.degrees(angle_rad)\n",
        "\n",
        "    # Hundred Lift Angle = 180도 (일직선) - 계산된 사이각\n",
        "    return 180 - angle_deg\n",
        "\n",
        "# --- 2. 분석 함수 정의 ---\n",
        "def calculate_hundred_metrics(df):\n",
        "    \"\"\"\n",
        "    Hundred 지표 (각도, 높이)를 계산\n",
        "    [수정] 다리 각도 계산을 '몸통 기준'으로 변경 (Relative Angle)\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # (1) 다리 선택 (잘 보이는 쪽으로)\n",
        "    if 'Left_Ankle_conf' not in df.columns or 'Right_Ankle_conf' not in df.columns:\n",
        "        use_left_leg_mask = True\n",
        "    else:\n",
        "        use_left_leg_mask = df['Left_Ankle_conf'] >= df['Right_Ankle_conf']\n",
        "\n",
        "    # 스마트 좌표 추출\n",
        "    smart_ankle_x = np.where(use_left_leg_mask, df['norm_Left_Ankle_x'], df['norm_Right_Ankle_x'])\n",
        "    smart_ankle_y = np.where(use_left_leg_mask, df['norm_Left_Ankle_y'], df['norm_Right_Ankle_y'])\n",
        "    smart_hip_x = np.where(use_left_leg_mask, df['norm_Left_Hip_x'], df['norm_Right_Hip_x'])\n",
        "    smart_hip_y = np.where(use_left_leg_mask, df['norm_Left_Hip_y'], df['norm_Right_Hip_y'])\n",
        "\n",
        "    # (1_A) [핵심 수정] 다리 높이 각도 (몸통 기준)\n",
        "\n",
        "    # 1. Neck Center 좌표 생성 (Vector A의 기준)\n",
        "    shoulder_center_x = (df['norm_Left_Shoulder_x'] + df['norm_Right_Shoulder_x']) / 2\n",
        "    shoulder_center_y = (df['norm_Left_Shoulder_y'] + df['norm_Right_Shoulder_y']) / 2\n",
        "\n",
        "    neck_coords = np.array([shoulder_center_x, shoulder_center_y]).T # (N, 2)\n",
        "\n",
        "    # 2. 스마트 Hip/Ankle 좌표 배열\n",
        "    smart_hip_coords = np.array([smart_hip_x, smart_hip_y]).T\n",
        "    smart_ankle_coords = np.array([smart_ankle_x, smart_ankle_y]).T\n",
        "\n",
        "    # 3. Row-wise로 각도 계산 및 리스트화 (Dot Product는 벡터화가 어려움)\n",
        "    angle_list = []\n",
        "    for i in range(len(df)):\n",
        "        hip = smart_hip_coords[i]\n",
        "        ankle = smart_ankle_coords[i]\n",
        "        neck = neck_coords[i]\n",
        "\n",
        "        angle_val = get_relative_lift_angle(neck, hip, ankle)\n",
        "        angle_list.append(angle_val)\n",
        "\n",
        "    df_out['Leg_Height_Angle'] = angle_list # 새로운 각도 할당\n",
        "\n",
        "    # (2) 코어 고정각 (Shoulder-Hip-Knee) - 이 부분은 원리상 그대로 사용\n",
        "    def get_angle(a, b, c):\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
        "\n",
        "    p_shoulder = df[['norm_Left_Shoulder_x', 'norm_Left_Shoulder_y']].values\n",
        "    p_hip = df[['norm_Left_Hip_x', 'norm_Left_Hip_y']].values\n",
        "    p_knee = df[['norm_Left_Knee_x', 'norm_Left_Knee_y']].values\n",
        "\n",
        "    # 코어 각도 계산은 원래 3점 사이의 각도이므로 기존 방식 유지\n",
        "    df_out['Core_Angle'] = [get_angle(s, h, k) if not (np.isnan(s).any() or np.isnan(h).any() or np.isnan(k).any()) else np.nan\n",
        "                            for s, h, k in zip(p_shoulder, p_hip, p_knee)]\n",
        "\n",
        "    # (3) 상체 컬업 (Nose가 Shoulder Center보다 얼마나 위에 있나) - 유지\n",
        "    shoulder_center_y = (df['norm_Left_Shoulder_y'] + df['norm_Right_Shoulder_y']) / 2\n",
        "    df_out['Head_Curl_Position'] = shoulder_center_y - df['norm_Nose_y']\n",
        "\n",
        "    return df_out\n",
        "\n",
        "def analyze_stability(group):\n",
        "    # (이하 analyze_stability 함수와 메인 실행부는 변경하지 않고 그대로 유지됩니다.)\n",
        "    # ... (생략) ...\n",
        "\n",
        "    # --- [보간(Interpolation) 전처리] ---\n",
        "    parts_to_clean = ['Left_Ankle', 'Right_Ankle', 'Left_Hip', 'Right_Hip',\n",
        "                      'Left_Shoulder', 'Right_Shoulder', 'Nose', 'Left_Knee']\n",
        "    group_clean = group.copy()\n",
        "    for part in parts_to_clean:\n",
        "        if f'{part}_conf' in group_clean.columns:\n",
        "            low_conf_mask = group_clean[f'{part}_conf'] < 0.5\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_x'] = np.nan\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_y'] = np.nan\n",
        "    norm_cols = [col for col in group_clean.columns if col.startswith('norm_')]\n",
        "    group_clean[norm_cols] = group_clean[norm_cols].interpolate(method='linear', limit_direction='both', limit=5)\n",
        "\n",
        "    # 1. 'Hundred' 지표 계산\n",
        "    group_with_metrics = calculate_hundred_metrics(group_clean)\n",
        "    group_with_metrics = group_with_metrics.dropna(subset=['Leg_Height_Angle', 'Head_Curl_Position'])\n",
        "\n",
        "    if group_with_metrics.empty:\n",
        "        return None\n",
        "\n",
        "    # --- [Longest Streak 로직] ---\n",
        "    group_with_metrics['is_candidate'] = (\n",
        "        (group_with_metrics['Leg_Height_Angle'] > 5) &\n",
        "        (group_with_metrics['Leg_Height_Angle'] < 45) &\n",
        "        (group_with_metrics['Head_Curl_Position'] > 0.05)\n",
        "    )\n",
        "\n",
        "    group_with_metrics['block_id'] = (group_with_metrics['is_candidate'] != group_with_metrics['is_candidate'].shift()).cumsum()\n",
        "    candidate_streaks = group_with_metrics[group_with_metrics['is_candidate'] == True].groupby('block_id').size()\n",
        "\n",
        "    if candidate_streaks.empty:\n",
        "        return None\n",
        "\n",
        "    longest_block_id = candidate_streaks.idxmax()\n",
        "    active_frames = group_with_metrics[\n",
        "        (group_with_metrics['block_id'] == longest_block_id) &\n",
        "        (group_with_metrics['is_candidate'] == True)\n",
        "    ].copy()\n",
        "    # --- [Longest Streak 로직 끝] ---\n",
        "\n",
        "    if len(active_frames) < 10:\n",
        "        return None\n",
        "\n",
        "    stability_score = active_frames['Leg_Height_Angle'].std()\n",
        "    core_stability = active_frames['Core_Angle'].std()\n",
        "\n",
        "    if len(active_frames['Frame'].unique()) > 1:\n",
        "        slope, _ = np.polyfit(active_frames['Frame'], active_frames['Leg_Height_Angle'], 1)\n",
        "    else:\n",
        "        slope = 0\n",
        "\n",
        "    start_frame = active_frames['Frame'].min()\n",
        "    end_frame = active_frames['Frame'].max()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Leg_Stability_StdDev': stability_score,\n",
        "        'Core_Stability_StdDev': core_stability,\n",
        "        'Leg_Fatigue_Slope': slope,\n",
        "        'Avg_Leg_Angle': active_frames['Leg_Height_Angle'].mean(),\n",
        "        'Avg_Core_Angle': active_frames['Core_Angle'].mean(),\n",
        "        'Active_Frame_Count': len(active_frames),\n",
        "        'Active_Start_Frame': start_frame,\n",
        "        'Active_End_Frame': end_frame\n",
        "    })\n",
        "\n",
        "\n",
        "# --- 3. 메인 실행부 (Loop 방식) ---\n",
        "\n",
        "# [경로 설정]\n",
        "drive_root = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/dataset/Hundred'\n",
        "norm_folder_path = os.path.join(drive_root, 'keypoint_Hundred', 'norm')\n",
        "all_files = glob.glob(os.path.join(norm_folder_path, \"norm_*.csv\"))\n",
        "\n",
        "output_path = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis'\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"❌ '{norm_folder_path}' 경로에 'norm_*.csv' 파일이 없습니다. 경로를 확인해주세요.\")\n",
        "else:\n",
        "    print(f\"✅ 총 {len(all_files)}개의 정규화된 CSV 파일을 분석합니다.\")\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for i, file_path in enumerate(all_files):\n",
        "        filename = os.path.basename(file_path)\n",
        "        print(f\"  [{i+1}/{len(all_files)}] 분석 중: {filename}\")\n",
        "\n",
        "        try:\n",
        "            # 1. 파일명에서 메타데이터(ID, Level) 추출 (안전한 방식으로 수정)\n",
        "            clean_name = filename.replace('norm_', '').replace('.csv', '')\n",
        "            parts = clean_name.split('_')\n",
        "\n",
        "            place = parts[2]\n",
        "            level = \"Unknown\"\n",
        "            person_id = \"Unknown\"\n",
        "            pose_name = \"Unknown\"\n",
        "\n",
        "            # 키워드로 검색 (순서가 바뀌어도 OK)\n",
        "            for part in parts:\n",
        "                if part.startswith('actor'):\n",
        "                    person_id = part\n",
        "                if part in ['고급', '중급', '초급']:\n",
        "                    level = part\n",
        "                if part.lower() == 'hundred':\n",
        "                    pose_name = 'Hundred'\n",
        "\n",
        "            # 'Hundred' 파일이 아니면 건너뛰기\n",
        "            if pose_name != 'Hundred':\n",
        "                print(f\"    ... [Skip] 'Hundred' 파일이 아니므로 건너뜁니다.\")\n",
        "                continue\n",
        "\n",
        "            # 2. CSV 읽기\n",
        "            df_single_file = pd.read_csv(file_path)\n",
        "\n",
        "            # 3. '안정성' 분석 (보간 + 지표 계산)\n",
        "            stability_summary = analyze_stability(df_single_file)\n",
        "\n",
        "            # 4. 결과 저장\n",
        "            if stability_summary is not None:\n",
        "                stability_summary['Source_File'] = filename\n",
        "                stability_summary['Person_ID'] = person_id\n",
        "                stability_summary['Level'] = level\n",
        "                stability_summary['Place'] = place\n",
        "                results_list.append(stability_summary)\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"    ⚠️ {filename} 처리 중 오류 발생 (KeyError): {e} - CSV에 필요한 컬럼이 없습니다.\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ {filename} 처리 중 알 수 없는 오류 발생: {e}\")\n",
        "\n",
        "    # --- 4. 최종 결과 저장 (오류 수정된 버전) ---\n",
        "    if results_list:\n",
        "        df_final_eda = pd.DataFrame(results_list)\n",
        "\n",
        "        output_dir = os.path.join(output_path, \"keypoint_Hundred_Analysis_vFinal\") # 새 폴더 이름\n",
        "        save_path = os.path.join(output_dir, \"EDA_Hundred_Stability_Report_vFinal_BodyRelative.csv\") # 파일명 변경\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        df_final_eda.to_csv(save_path, index=False)\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\"✅ 'Hundred' 안정성 분석 완료! (몸통 기준 각도 적용)\")\n",
        "        print(f\"저장된 파일: {save_path}\")\n",
        "        print(f\"총 {len(df_final_eda)}개의 'Hundred' 영상이 분석되었습니다.\")\n",
        "\n",
        "        print(\"\\n[결과 미리보기]\")\n",
        "        print(df_final_eda.head())\n",
        "    else:\n",
        "        print(\"분석할 'Hundred' 영상이 없거나 유효한 데이터가 없습니다.\")"
      ],
      "metadata": {
        "id": "QYnji3nhfM6O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e0a26ef-6985-49fa-9016-e34138e1ea9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ 총 117개의 정규화된 CSV 파일을 분석합니다.\n",
            "  [1/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.51.54_CAM_1.csv\n",
            "  [2/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.16.15_CAM_1.csv\n",
            "  [3/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.14.40_CAM_1.csv\n",
            "  [4/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.15.35_CAM_1.csv\n",
            "  [5/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.17.02_CAM_1.csv\n",
            "  [6/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP076_20220929_15.26.04_CAM_1.csv\n",
            "  [7/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.10.42_CAM_1.csv\n",
            "  [8/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.11.30_CAM_1.csv\n",
            "  [9/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.12.19_CAM_1.csv\n",
            "  [10/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.01.17_CAM_1.csv\n",
            "  [11/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.02.15_CAM_1.csv\n",
            "  [12/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.03.10_CAM_1.csv\n",
            "  [13/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.50.10_CAM_1.csv\n",
            "  [14/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.51.01_CAM_1.csv\n",
            "  [15/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.52.49_CAM_1.csv\n",
            "  [16/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.53.42_CAM_1.csv\n",
            "  [17/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.35.27_CAM_1.csv\n",
            "  [18/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.36.16_CAM_1.csv\n",
            "  [19/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.37.05_CAM_1.csv\n",
            "  [20/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.37.52_CAM_1.csv\n",
            "  [21/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.38.46_CAM_1.csv\n",
            "  [22/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.39.39_CAM_1.csv\n",
            "  [23/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP092_20221005_11.17.31_CAM_1.csv\n",
            "  [24/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.36.09_CAM_1.csv\n",
            "  [25/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.37.03_CAM_1.csv\n",
            "  [26/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.37.57_CAM_1.csv\n",
            "  [27/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.38.54_CAM_1.csv\n",
            "  [28/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.39.44_CAM_1.csv\n",
            "  [29/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.40.59_CAM_1.csv\n",
            "  [30/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.42.02_CAM_1.csv\n",
            "  [31/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.42.56_CAM_1.csv\n",
            "  [32/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.43.45_CAM_1.csv\n",
            "  [33/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.16.59_CAM_1.csv\n",
            "  [34/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.17.46_CAM_1.csv\n",
            "  [35/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.18.37_CAM_1.csv\n",
            "  [36/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.19.24_CAM_1.csv\n",
            "  [37/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.20.17_CAM_1.csv\n",
            "  [38/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.22.53_CAM_1.csv\n",
            "  [39/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.26.00_CAM_1.csv\n",
            "  [40/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.13.16_CAM_1.csv\n",
            "  [41/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.14.22_CAM_1.csv\n",
            "  [42/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.15.20_CAM_1.csv\n",
            "  [43/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.16.13_CAM_1.csv\n",
            "  [44/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.17.17_CAM_1.csv\n",
            "  [45/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.18.38_CAM_1.csv\n",
            "  [46/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.19.28_CAM_1.csv\n",
            "  [47/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.01.06_CAM_1.csv\n",
            "  [48/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.01.55_CAM_1.csv\n",
            "  [49/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.02.45_CAM_1.csv\n",
            "  [50/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.03.39_CAM_1.csv\n",
            "  [51/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.04.44_CAM_1.csv\n",
            "  [52/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.20.51_CAM_1.csv\n",
            "  [53/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.21.48_CAM_1.csv\n",
            "  [54/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.15.09_CAM_1.csv\n",
            "  [55/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.17.09_CAM_1.csv\n",
            "  [56/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.18.05_CAM_1.csv\n",
            "  [57/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.05.59_CAM_1.csv\n",
            "  [58/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.36.07_CAM_1.csv\n",
            "  [59/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.36.53_CAM_1.csv\n",
            "  [60/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.37.42_CAM_1.csv\n",
            "  [61/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.38.28_CAM_1.csv\n",
            "  [62/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.39.18_CAM_1.csv\n",
            "  [63/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.40.12_CAM_1.csv\n",
            "  [64/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.40.59_CAM_1.csv\n",
            "  [65/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.41.49_CAM_1.csv\n",
            "  [66/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.42.35_CAM_1.csv\n",
            "  [67/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.43.23_CAM_1.csv\n",
            "  [68/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_15.11.39_CAM_1.csv\n",
            "  [69/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.04.49_CAM_1.csv\n",
            "  [70/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.07.10_CAM_1.csv\n",
            "  [71/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.08.25_CAM_1.csv\n",
            "  [72/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.10.55_CAM_1.csv\n",
            "  [73/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.12.05_CAM_1.csv\n",
            "  [74/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.13.11_CAM_1.csv\n",
            "  [75/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.40.24_CAM_1.csv\n",
            "  [76/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.41.26_CAM_1.csv\n",
            "  [77/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.42.27_CAM_1.csv\n",
            "  [78/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.43.26_CAM_1.csv\n",
            "  [79/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.44.29_CAM_1.csv\n",
            "  [80/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.45.20_CAM_1.csv\n",
            "  [81/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.46.08_CAM_1.csv\n",
            "  [82/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.15.49_CAM_1.csv\n",
            "  [83/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.16.51_CAM_1.csv\n",
            "  [84/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.18.00_CAM_1.csv\n",
            "  [85/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.19.05_CAM_1.csv\n",
            "  [86/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.19.49_CAM_1.csv\n",
            "  [87/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.20.45_CAM_1.csv\n",
            "  [88/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.21.38_CAM_1.csv\n",
            "  [89/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.07.30_CAM_1.csv\n",
            "  [90/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.08.20_CAM_1.csv\n",
            "  [91/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.09.19_CAM_1.csv\n",
            "  [92/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.10.25_CAM_1.csv\n",
            "  [93/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.11.17_CAM_1.csv\n",
            "  [94/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.12.07_CAM_1.csv\n",
            "  [95/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.12.55_CAM_1.csv\n",
            "  [96/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.13.47_CAM_1.csv\n",
            "  [97/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.30.00_CAM_1.csv\n",
            "  [98/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.30.46_CAM_1.csv\n",
            "  [99/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.31.33_CAM_1.csv\n",
            "  [100/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.07.14_CAM_1.csv\n",
            "  [101/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.08.04_CAM_1.csv\n",
            "  [102/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.08.53_CAM_1.csv\n",
            "  [103/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.09.55_CAM_1.csv\n",
            "  [104/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.10.45_CAM_1.csv\n",
            "  [105/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.11.37_CAM_1.csv\n",
            "  [106/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.12.29_CAM_1.csv\n",
            "  [107/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.13.19_CAM_1.csv\n",
            "  [108/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.22.43_CAM_1.csv\n",
            "  [109/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.23.38_CAM_1.csv\n",
            "  [110/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.24.32_CAM_1.csv\n",
            "  [111/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.25.26_CAM_1.csv\n",
            "  [112/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.26.20_CAM_1.csv\n",
            "  [113/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.27.16_CAM_1.csv\n",
            "  [114/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.52.51_CAM_1.csv\n",
            "  [115/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.54.00_CAM_1.csv\n",
            "  [116/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.54.55_CAM_1.csv\n",
            "  [117/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.55.52_CAM_1.csv\n",
            "--------------------------------------------------\n",
            "✅ 'Hundred' 안정성 분석 완료! (몸통 기준 각도 적용)\n",
            "저장된 파일: /content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis/keypoint_Hundred_Analysis_vFinal/EDA_Hundred_Stability_Report_vFinal_BodyRelative.csv\n",
            "총 117개의 'Hundred' 영상이 분석되었습니다.\n",
            "\n",
            "[결과 미리보기]\n",
            "   Leg_Stability_StdDev  Core_Stability_StdDev  Leg_Fatigue_Slope  \\\n",
            "0              8.988517               9.388775          -0.065948   \n",
            "1              2.707412               6.399088          -0.025429   \n",
            "2              6.235150               7.328788          -0.011494   \n",
            "3              7.492381               9.290188          -0.017038   \n",
            "4              7.097681               8.219723          -0.040306   \n",
            "\n",
            "   Avg_Leg_Angle  Avg_Core_Angle  Active_Frame_Count  Active_Start_Frame  \\\n",
            "0      30.350666      152.597532               162.0               255.0   \n",
            "1      13.510018      159.192815               136.0               282.0   \n",
            "2      18.939020      160.691995               180.0               207.0   \n",
            "3      25.428389      150.122644               181.0               226.0   \n",
            "4      24.836891      151.546235               153.0               231.0   \n",
            "\n",
            "   Active_End_Frame                                        Source_File  \\\n",
            "0             416.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "1             417.0  norm_필라테스_가산_C_Mat_Hundred_고급_actor...   \n",
            "2             386.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "3             406.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "4             383.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "\n",
            "   Person_ID    Level Place  \n",
            "0  actorP082  Unknown     A  \n",
            "1  actorP079  Unknown     C  \n",
            "2  actorP075  Unknown     A  \n",
            "3  actorP075  Unknown     A  \n",
            "4  actorP075  Unknown     A  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from scipy.stats import linregress\n",
        "from sklearn.cluster import KMeans # [추가] 군집화를 위한 라이브러리\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. 드라이브 마운트\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# --- 2. 분석 함수 정의 ---\n",
        "\n",
        "def calculate_hundred_metrics(df):\n",
        "    \"\"\"\n",
        "    [유지] Hundred 지표 (각도, 높이) 계산 함수\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # (1) 다리 선택 (잘 보이는 쪽으로)\n",
        "    if 'Left_Ankle_conf' not in df.columns or 'Right_Ankle_conf' not in df.columns:\n",
        "        use_left_leg_mask = True\n",
        "    else:\n",
        "        use_left_leg_mask = df['Left_Ankle_conf'] >= df['Right_Ankle_conf']\n",
        "\n",
        "    smart_ankle_x = np.where(use_left_leg_mask, df['norm_Left_Ankle_x'], df['norm_Right_Ankle_x'])\n",
        "    smart_ankle_y = np.where(use_left_leg_mask, df['norm_Left_Ankle_y'], df['norm_Right_Ankle_y'])\n",
        "    smart_hip_x = np.where(use_left_leg_mask, df['norm_Left_Hip_x'], df['norm_Right_Hip_x'])\n",
        "    smart_hip_y = np.where(use_left_leg_mask, df['norm_Left_Hip_y'], df['norm_Right_Hip_y'])\n",
        "\n",
        "    # (1_A) 다리 높이 각도 (Smart Leg 기준)\n",
        "    dy = -(smart_ankle_y - smart_hip_y)\n",
        "    dx = np.abs(smart_ankle_x - smart_hip_x)\n",
        "    df_out['Leg_Height_Angle'] = np.degrees(np.arctan2(dy, dx))\n",
        "\n",
        "    # (2) 코어 고정각\n",
        "    def get_angle(a, b, c):\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
        "\n",
        "    p_shoulder = df[['norm_Left_Shoulder_x', 'norm_Left_Shoulder_y']].values\n",
        "    p_hip = df[['norm_Left_Hip_x', 'norm_Left_Hip_y']].values\n",
        "    p_knee = df[['norm_Left_Knee_x', 'norm_Left_Knee_y']].values\n",
        "\n",
        "    df_out['Core_Angle'] = [get_angle(s, h, k) if not (np.isnan(s).any() or np.isnan(h).any() or np.isnan(k).any()) else np.nan\n",
        "                            for s, h, k in zip(p_shoulder, p_hip, p_knee)]\n",
        "\n",
        "    # (3) 상체 컬업\n",
        "    shoulder_center_y = (df['norm_Left_Shoulder_y'] + df['norm_Right_Shoulder_y']) / 2\n",
        "    df_out['Head_Curl_Position'] = shoulder_center_y - df['norm_Nose_y']\n",
        "\n",
        "    return df_out\n",
        "\n",
        "def analyze_stability(group):\n",
        "    \"\"\"\n",
        "    [전면 수정] K-Means 군집화를 이용해 'Active Frame' 자동 감지\n",
        "    \"\"\"\n",
        "\n",
        "    # --- [1. 보간(Interpolation) 전처리] ---\n",
        "    parts_to_clean = ['Left_Ankle', 'Right_Ankle', 'Left_Hip', 'Right_Hip',\n",
        "                      'Left_Shoulder', 'Right_Shoulder', 'Nose', 'Left_Knee']\n",
        "    group_clean = group.copy()\n",
        "    for part in parts_to_clean:\n",
        "        if f'{part}_conf' in group_clean.columns:\n",
        "            low_conf_mask = group_clean[f'{part}_conf'] < 0.5\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_x'] = np.nan\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_y'] = np.nan\n",
        "\n",
        "    norm_cols = [col for col in group_clean.columns if col.startswith('norm_')]\n",
        "    group_clean[norm_cols] = group_clean[norm_cols].interpolate(method='linear', limit_direction='both', limit=5)\n",
        "\n",
        "    # --- [2. 지표 계산] ---\n",
        "    group_with_metrics = calculate_hundred_metrics(group_clean)\n",
        "\n",
        "    # 분석을 위해 NaN이 있는 행은 잠시 제외 (K-Means 오류 방지)\n",
        "    valid_data = group_with_metrics.dropna(subset=['Leg_Height_Angle', 'Head_Curl_Position']).copy()\n",
        "\n",
        "    if len(valid_data) < 30: # 데이터가 너무 적으면 분석 불가\n",
        "        return None\n",
        "\n",
        "    # --- [3. K-Means 군집화로 Active 구간 찾기] ---\n",
        "    # 데이터를 2개 그룹(휴식 vs 운동)으로 나눔\n",
        "    X = valid_data[['Leg_Height_Angle']].values\n",
        "\n",
        "    try:\n",
        "        # k=2로 군집화 (Random State 고정으로 재현성 확보)\n",
        "        kmeans = KMeans(n_clusters=2, n_init=10, random_state=42)\n",
        "        valid_data['cluster'] = kmeans.fit_predict(X)\n",
        "\n",
        "        # 각 군집의 평균 각도 계산\n",
        "        cluster_means = valid_data.groupby('cluster')['Leg_Height_Angle'].mean()\n",
        "\n",
        "        # 평균 각도가 더 높은 군집을 'Active' 군집으로 선정\n",
        "        active_cluster_id = cluster_means.idxmax()\n",
        "        active_mean_angle = cluster_means.max()\n",
        "\n",
        "        # [최소 조건] 아무리 높아도 평균 5도 미만이면 운동 안 한 것으로 간주\n",
        "        if active_mean_angle < 5:\n",
        "            return None\n",
        "\n",
        "        # Active 군집인 프레임에만 True 표시\n",
        "        valid_data['is_candidate'] = valid_data['cluster'] == active_cluster_id\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"K-Means Error: {e}\")\n",
        "        return None\n",
        "\n",
        "    # --- [4. 가장 긴 연속 구간(Longest Streak) 추출] ---\n",
        "    # 군집화 결과에서 노이즈를 제거하고 가장 길게 유지한 구간만 선택\n",
        "\n",
        "    # 연속된 그룹 ID 부여\n",
        "    valid_data['block_id'] = (valid_data['is_candidate'] != valid_data['is_candidate'].shift()).cumsum()\n",
        "\n",
        "    # 'True(Active)'인 블록들의 길이 계산\n",
        "    candidate_streaks = valid_data[valid_data['is_candidate'] == True].groupby('block_id').size()\n",
        "\n",
        "    if candidate_streaks.empty:\n",
        "        return None\n",
        "\n",
        "    # 가장 긴 블록 선택\n",
        "    longest_block_id = candidate_streaks.idxmax()\n",
        "\n",
        "    # 최종 Active Frames 추출\n",
        "    active_frames = valid_data[\n",
        "        (valid_data['block_id'] == longest_block_id) &\n",
        "        (valid_data['is_candidate'] == True)\n",
        "    ].copy()\n",
        "\n",
        "    if len(active_frames) < 10:\n",
        "        return None\n",
        "\n",
        "    # --- [5. 최종 통계 산출] ---\n",
        "    stability_score = active_frames['Leg_Height_Angle'].std()\n",
        "    core_stability = active_frames['Core_Angle'].std()\n",
        "\n",
        "    if len(active_frames['Frame'].unique()) > 1:\n",
        "        slope, _ = np.polyfit(active_frames['Frame'], active_frames['Leg_Height_Angle'], 1)\n",
        "    else:\n",
        "        slope = 0\n",
        "\n",
        "    start_frame = active_frames['Frame'].min()\n",
        "    end_frame = active_frames['Frame'].max()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Leg_Stability_StdDev': stability_score,\n",
        "        'Core_Stability_StdDev': core_stability,\n",
        "        'Leg_Fatigue_Slope': slope,\n",
        "        'Avg_Leg_Angle': active_frames['Leg_Height_Angle'].mean(),\n",
        "        'Avg_Core_Angle': active_frames['Core_Angle'].mean(),\n",
        "        'Active_Frame_Count': len(active_frames),\n",
        "        'Active_Start_Frame': start_frame,\n",
        "        'Active_End_Frame': end_frame\n",
        "    })\n",
        "\n",
        "# --- 3. 메인 실행부 (Loop 방식) ---\n",
        "\n",
        "# [경로 설정]\n",
        "drive_root = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/dataset/Hundred'\n",
        "norm_folder_path = os.path.join(drive_root, 'norm')\n",
        "all_files = glob.glob(os.path.join(norm_folder_path, \"norm_*.csv\"))\n",
        "\n",
        "output_path = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis'\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"❌ '{norm_folder_path}' 경로에 'norm_*.csv' 파일이 없습니다.\")\n",
        "else:\n",
        "    print(f\"✅ 총 {len(all_files)}개의 파일을 분석합니다 (K-Means 적용).\")\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for i, file_path in enumerate(all_files):\n",
        "        filename = os.path.basename(file_path)\n",
        "        # 진행 상황 출력 (너무 많으면 줄이셔도 됩니다)\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  [{i+1}/{len(all_files)}] 처리 중...\")\n",
        "\n",
        "        try:\n",
        "            # 메타데이터 추출\n",
        "            clean_name = filename.replace('norm_', '').replace('.csv', '')\n",
        "            parts = clean_name.split('_')\n",
        "\n",
        "            place = parts[2]\n",
        "            level = parts[5]\n",
        "            person_id = parts[6]\n",
        "            pose_name = parts[4]\n",
        "\n",
        "            for part in parts:\n",
        "                if part.startswith('actor'): person_id = part\n",
        "                if part in ['고급', '중급', '초급']: level = part\n",
        "                if part.lower() == 'hundred': pose_name = 'Hundred'\n",
        "\n",
        "            if pose_name != 'Hundred':\n",
        "                continue\n",
        "\n",
        "            # 데이터 읽기 및 분석\n",
        "            df_single_file = pd.read_csv(file_path)\n",
        "            stability_summary = analyze_stability(df_single_file)\n",
        "\n",
        "            if stability_summary is not None:\n",
        "                stability_summary['Source_File'] = filename\n",
        "                stability_summary['Person_ID'] = person_id\n",
        "                stability_summary['Level'] = level\n",
        "                stability_summary['Place'] = place\n",
        "                results_list.append(stability_summary)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ {filename} 오류: {e}\")\n",
        "\n",
        "    # 저장\n",
        "    if results_list:\n",
        "        df_final_eda = pd.DataFrame(results_list)\n",
        "\n",
        "        output_dir = os.path.join(output_path, \"keypoint_Hundred_Analysis_KMeans\")\n",
        "        save_path = os.path.join(output_dir, \"EDA_Hundred_Stability_Report_KMeans.csv\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        df_final_eda.to_csv(save_path, index=False)\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\"✅ 'Hundred' K-Means 기반 분석 완료!\")\n",
        "        print(f\"저장된 파일: {save_path}\")\n",
        "        print(df_final_eda.head())\n",
        "    else:\n",
        "        print(\"분석 결과가 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40a-sQiks9S5",
        "outputId": "c1a2ea07-a151-4dae-a5dd-bd5645fbdbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ 총 117개의 파일을 분석합니다 (K-Means 적용).\n",
            "  [1/117] 처리 중...\n",
            "  [11/117] 처리 중...\n",
            "  [21/117] 처리 중...\n",
            "  [31/117] 처리 중...\n",
            "  [41/117] 처리 중...\n",
            "  [51/117] 처리 중...\n",
            "  [61/117] 처리 중...\n",
            "  [71/117] 처리 중...\n",
            "  [81/117] 처리 중...\n",
            "  [91/117] 처리 중...\n",
            "  [101/117] 처리 중...\n",
            "  [111/117] 처리 중...\n",
            "--------------------------------------------------\n",
            "✅ 'Hundred' K-Means 기반 분석 완료!\n",
            "저장된 파일: /content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis/keypoint_Hundred_Analysis_KMeans/EDA_Hundred_Stability_Report_KMeans.csv\n",
            "   Leg_Stability_StdDev  Core_Stability_StdDev  Leg_Fatigue_Slope  \\\n",
            "0             13.858505              27.198207           0.014282   \n",
            "1             10.132320              30.215632           0.021984   \n",
            "\n",
            "   Avg_Leg_Angle  Avg_Core_Angle  Active_Frame_Count  Active_Start_Frame  \\\n",
            "0       5.434976      153.699158               386.0               134.0   \n",
            "1       5.009372      156.102235               409.0               101.0   \n",
            "\n",
            "   Active_End_Frame                                        Source_File  \\\n",
            "0             519.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "1             509.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "\n",
            "   Person_ID  Level Place  \n",
            "0  actorP082  고급     A  \n",
            "1  actorP093  고급     A  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from scipy.stats import linregress\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. 드라이브 마운트 (최초 1회 실행)\n",
        "\n",
        "# --- 2. 분석 함수 정의 ---\n",
        "def calculate_hundred_metrics(df):\n",
        "    \"\"\"\n",
        "    Hundred 지표 (각도, 높이)를 계산\n",
        "    - (Smart Leg) 신뢰도 높은 다리 자동 선택\n",
        "    - (abs(dx)) 방향에 상관없이 예각(Angle) 계산\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # (1) 다리 선택 (잘 보이는 쪽으로)\n",
        "    # 원본 _conf 컬럼이 있는지 확인\n",
        "    if 'Left_Ankle_conf' not in df.columns or 'Right_Ankle_conf' not in df.columns:\n",
        "        print(\"    ⚠️ Ankle_conf 컬럼이 없어 '왼쪽 다리'를 기준으로 계산합니다.\")\n",
        "        use_left_leg_mask = True # conf 없으면 왼쪽 기준\n",
        "    else:\n",
        "        use_left_leg_mask = df['Left_Ankle_conf'] >= df['Right_Ankle_conf']\n",
        "\n",
        "    smart_ankle_x = np.where(use_left_leg_mask, df['norm_Left_Ankle_x'], df['norm_Right_Ankle_x'])\n",
        "    smart_ankle_y = np.where(use_left_leg_mask, df['norm_Left_Ankle_y'], df['norm_Right_Ankle_y'])\n",
        "    smart_hip_x = np.where(use_left_leg_mask, df['norm_Left_Hip_x'], df['norm_Right_Hip_x'])\n",
        "    smart_hip_y = np.where(use_left_leg_mask, df['norm_Left_Hip_y'], df['norm_Right_Hip_y'])\n",
        "\n",
        "    # (1_A) 다리 높이 각도 (Smart Leg 기준)\n",
        "    dy = -(smart_ankle_y - smart_hip_y) # Y축 반전\n",
        "    dx = np.abs(smart_ankle_x - smart_hip_x) # [수정] 절댓값 적용\n",
        "    df_out['Leg_Height_Angle'] = np.degrees(np.arctan2(dy, dx))\n",
        "\n",
        "    # (2) 코어 고정각 (Shoulder-Hip-Knee)\n",
        "    def get_angle(a, b, c):\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
        "\n",
        "    p_shoulder = df[['norm_Left_Shoulder_x', 'norm_Left_Shoulder_y']].values\n",
        "    p_hip = df[['norm_Left_Hip_x', 'norm_Left_Hip_y']].values\n",
        "    p_knee = df[['norm_Left_Knee_x', 'norm_Left_Knee_y']].values\n",
        "\n",
        "    df_out['Core_Angle'] = [get_angle(s, h, k) if not (np.isnan(s).any() or np.isnan(h).any() or np.isnan(k).any()) else np.nan\n",
        "                            for s, h, k in zip(p_shoulder, p_hip, p_knee)]\n",
        "\n",
        "    # (3) 상체 컬업 (Nose가 Shoulder Center보다 얼마나 위에 있나)\n",
        "    shoulder_center_y = (df['norm_Left_Shoulder_y'] + df['norm_Right_Shoulder_y']) / 2\n",
        "    df_out['Head_Curl_Position'] = shoulder_center_y - df['norm_Nose_y']\n",
        "\n",
        "    return df_out\n",
        "\n",
        "def analyze_stability(group):\n",
        "    \"\"\"\n",
        "    [심플 버전] 'Hundred'는 누워서 하는 운동이다!\n",
        "    -> 코(Nose)가 '누워있는 높이'에 있을 때만 잘라내서 분석\n",
        "    \"\"\"\n",
        "\n",
        "    # --- [1. 보간(Interpolation) 전처리] ---\n",
        "    # (데이터 끊김 방지용 필수 코드는 유지)\n",
        "    parts_to_clean = ['Left_Ankle', 'Right_Ankle', 'Left_Hip', 'Right_Hip',\n",
        "                      'Left_Shoulder', 'Right_Shoulder', 'Nose', 'Left_Knee']\n",
        "    group_clean = group.copy()\n",
        "    for part in parts_to_clean:\n",
        "        if f'{part}_conf' in group_clean.columns:\n",
        "            low_conf_mask = group_clean[f'{part}_conf'] < 0.5\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_x'] = np.nan\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_y'] = np.nan\n",
        "    norm_cols = [col for col in group_clean.columns if col.startswith('norm_')]\n",
        "    group_clean[norm_cols] = group_clean[norm_cols].interpolate(method='linear', limit_direction='both', limit=5)\n",
        "\n",
        "    # 1. 지표 계산\n",
        "    group_with_metrics = calculate_hundred_metrics(group_clean)\n",
        "\n",
        "    # --- [2. 핵심 수정] Nose_Y 기반 단순 컷팅 ---\n",
        "\n",
        "    # (1) 코의 높이 범위 계산\n",
        "    min_nose_y = group_with_metrics['norm_Nose_y'].min() # 제일 높을 때 (앉음)\n",
        "    max_nose_y = group_with_metrics['norm_Nose_y'].max() # 제일 낮을 때 (누움)\n",
        "\n",
        "    # (2) \"누움\"의 기준선(Threshold) 정하기\n",
        "    # \"전체 높이 변화의 하위 40% 지점보다 더 아래(Max쪽에 가까움)에 있으면 누운 것이다\"\n",
        "    # (이미지 좌표계는 아래로 갈수록 값이 커지므로 max에 가까워야 누운 것)\n",
        "    threshold = max_nose_y - (max_nose_y - min_nose_y) * 0.4\n",
        "\n",
        "    # (3) 기준선보다 아래(누워있음)인 프레임만 선택\n",
        "    active_frames = group_with_metrics[\n",
        "        group_with_metrics['norm_Nose_y'] > threshold\n",
        "    ].copy()\n",
        "\n",
        "    # (4) 노이즈 제거: 너무 짧은 구간(1초 미만)은 무시하고 가장 긴 덩어리만 선택\n",
        "    # (앉았다가 잠깐 눕는 척 하거나 튀는 값 방지)\n",
        "    if active_frames.empty: return None\n",
        "\n",
        "    # 연속 구간 ID 부여\n",
        "    active_frames['frame_diff'] = active_frames['Frame'].diff()\n",
        "    active_frames['block_id'] = (active_frames['frame_diff'] > 5).cumsum() # 5프레임 이상 끊기면 다른 블록\n",
        "\n",
        "    # 가장 긴 블록 선택\n",
        "    longest_block_id = active_frames.groupby('block_id').size().idxmax()\n",
        "    final_active_frames = active_frames[active_frames['block_id'] == longest_block_id]\n",
        "\n",
        "    if len(final_active_frames) < 30: # 1초(30프레임)도 안 누워있으면 무시\n",
        "        return None\n",
        "\n",
        "    # --- [3. 최종 통계 산출] ---\n",
        "    stability_score = final_active_frames['Leg_Height_Angle'].std()\n",
        "    core_stability = final_active_frames['Core_Angle'].std()\n",
        "\n",
        "    if len(final_active_frames['Frame'].unique()) > 1:\n",
        "        slope, _ = np.polyfit(final_active_frames['Frame'], final_active_frames['Leg_Height_Angle'], 1)\n",
        "    else:\n",
        "        slope = 0\n",
        "\n",
        "    start_frame = final_active_frames['Frame'].min()\n",
        "    end_frame = final_active_frames['Frame'].max()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Leg_Stability_StdDev': stability_score,\n",
        "        'Core_Stability_StdDev': core_stability,\n",
        "        'Leg_Fatigue_Slope': slope,\n",
        "        'Avg_Leg_Angle': final_active_frames['Leg_Height_Angle'].mean(),\n",
        "        'Avg_Core_Angle': final_active_frames['Core_Angle'].mean(),\n",
        "        'Active_Frame_Count': len(final_active_frames),\n",
        "        'Active_Start_Frame': start_frame,\n",
        "        'Active_End_Frame': end_frame\n",
        "    })\n",
        "\n",
        "# --- 3. 메인 실행부 (Loop 방식) ---\n",
        "\n",
        "# [경로 설정]\n",
        "drive_root = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/dataset/Hundred'\n",
        "norm_folder_path = os.path.join(drive_root, 'norm')\n",
        "all_files = glob.glob(os.path.join(norm_folder_path, \"norm_*.csv\"))\n",
        "\n",
        "output_path = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis'\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"❌ '{norm_folder_path}' 경로에 'norm_*.csv' 파일이 없습니다. 경로를 확인해주세요.\")\n",
        "else:\n",
        "    print(f\"✅ 총 {len(all_files)}개의 정규화된 CSV 파일을 분석합니다.\")\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for i, file_path in enumerate(all_files):\n",
        "        filename = os.path.basename(file_path)\n",
        "        print(f\"  [{i+1}/{len(all_files)}] 분석 중: {filename}\")\n",
        "\n",
        "        try:\n",
        "            # 1. 파일명에서 메타데이터(ID, Level) 추출 (안전한 방식으로 수정)\n",
        "            clean_name = filename.replace('norm_', '').replace('.csv', '')\n",
        "            parts = clean_name.split('_')\n",
        "\n",
        "            place = parts[2]\n",
        "            level = parts[5]\n",
        "            person_id = parts[6]\n",
        "            pose_name = parts[4]\n",
        "\n",
        "            # 키워드로 검색 (순서가 바뀌어도 OK)\n",
        "            for part in parts:\n",
        "                if part.startswith('actor'):\n",
        "                    person_id = part\n",
        "                if part in ['고급', '중급', '초급']:\n",
        "                    level = part\n",
        "                if part.lower() == 'hundred':\n",
        "                    pose_name = 'Hundred'\n",
        "\n",
        "            # 'Hundred' 파일이 아니면 건너뛰기\n",
        "            if pose_name != 'Hundred':\n",
        "                print(f\"    ... [Skip] 'Hundred' 파일이 아니므로 건너뜁니다.\")\n",
        "                continue\n",
        "\n",
        "            # 2. CSV 읽기\n",
        "            df_single_file = pd.read_csv(file_path)\n",
        "\n",
        "            # 3. '안정성' 분석 (보간 + 지표 계산)\n",
        "            # (calculate_hundred_metrics는 analyze_stability 내부에서 호출됨)\n",
        "            stability_summary = analyze_stability(df_single_file)\n",
        "\n",
        "            # 4. 결과 저장\n",
        "            if stability_summary is not None:\n",
        "                stability_summary['Source_File'] = filename\n",
        "                stability_summary['Person_ID'] = person_id\n",
        "                stability_summary['Level'] = level\n",
        "                stability_summary['Place'] = place\n",
        "                results_list.append(stability_summary)\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"    ⚠️ {filename} 처리 중 오류 발생 (KeyError): {e} - CSV에 필요한 컬럼이 없습니다.\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ {filename} 처리 중 알 수 없는 오류 발생: {e}\")\n",
        "\n",
        "    # --- 4. 최종 결과 저장 (오류 수정된 버전) ---\n",
        "    if results_list:\n",
        "        df_final_eda = pd.DataFrame(results_list)\n",
        "\n",
        "        output_dir = os.path.join(output_path, \"keypoint_Hundred_Analysis_noseY_active\") # 새 폴더 이름\n",
        "        save_path = os.path.join(output_dir, \"EDA_Hundred_Stability_Report_noseY_1119.csv\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        df_final_eda.to_csv(save_path, index=False)\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\"✅ 'Hundred' 안정성 분석 완료! (모든 기능 통합)\")\n",
        "        print(f\"저장된 파일: {save_path}\")\n",
        "        print(f\"총 {len(df_final_eda)}개의 'Hundred' 영상이 분석되었습니다.\")\n",
        "\n",
        "        print(\"\\n[결과 미리보기]\")\n",
        "        print(df_final_eda.head())\n",
        "    else:\n",
        "        print(\"분석할 'Hundred' 영상이 없거나 유효한 데이터가 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqtPwDuStndC",
        "outputId": "62574a46-f2d3-47fd-a508-dc838bb2e550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 총 117개의 정규화된 CSV 파일을 분석합니다.\n",
            "  [1/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.14.40_CAM_1.csv\n",
            "  [2/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.15.35_CAM_1.csv\n",
            "  [3/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.17.02_CAM_1.csv\n",
            "  [4/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP076_20220929_15.26.04_CAM_1.csv\n",
            "  [5/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.10.42_CAM_1.csv\n",
            "  [6/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.11.30_CAM_1.csv\n",
            "  [7/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.12.19_CAM_1.csv\n",
            "  [8/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.01.17_CAM_1.csv\n",
            "  [9/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.02.15_CAM_1.csv\n",
            "  [10/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.03.10_CAM_1.csv\n",
            "  [11/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.50.10_CAM_1.csv\n",
            "  [12/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.51.01_CAM_1.csv\n",
            "  [13/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.51.54_CAM_1.csv\n",
            "  [14/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.52.49_CAM_1.csv\n",
            "  [15/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.53.42_CAM_1.csv\n",
            "  [16/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.35.27_CAM_1.csv\n",
            "  [17/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.36.16_CAM_1.csv\n",
            "  [18/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.37.05_CAM_1.csv\n",
            "  [19/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.37.52_CAM_1.csv\n",
            "  [20/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.38.46_CAM_1.csv\n",
            "  [21/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.39.39_CAM_1.csv\n",
            "  [22/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP092_20221005_11.17.31_CAM_1.csv\n",
            "  [23/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.36.07_CAM_1.csv\n",
            "  [24/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.36.53_CAM_1.csv\n",
            "  [25/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.37.42_CAM_1.csv\n",
            "  [26/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.38.28_CAM_1.csv\n",
            "  [27/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.39.18_CAM_1.csv\n",
            "  [28/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.40.12_CAM_1.csv\n",
            "  [29/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.40.59_CAM_1.csv\n",
            "  [30/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.41.49_CAM_1.csv\n",
            "  [31/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.42.35_CAM_1.csv\n",
            "  [32/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.43.23_CAM_1.csv\n",
            "  [33/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.36.09_CAM_1.csv\n",
            "  [34/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.37.03_CAM_1.csv\n",
            "  [35/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.37.57_CAM_1.csv\n",
            "  [36/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.38.54_CAM_1.csv\n",
            "  [37/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.30.00_CAM_1.csv\n",
            "  [38/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.40.24_CAM_1.csv\n",
            "  [39/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.39.44_CAM_1.csv\n",
            "  [40/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.30.46_CAM_1.csv\n",
            "  [41/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.41.26_CAM_1.csv\n",
            "  [42/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.31.33_CAM_1.csv\n",
            "  [43/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.40.59_CAM_1.csv\n",
            "  [44/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.42.27_CAM_1.csv\n",
            "  [45/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.07.14_CAM_1.csv\n",
            "  [46/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.42.02_CAM_1.csv\n",
            "  [47/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.43.26_CAM_1.csv\n",
            "  [48/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.08.04_CAM_1.csv\n",
            "  [49/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.42.56_CAM_1.csv\n",
            "  [50/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.44.29_CAM_1.csv\n",
            "  [51/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.08.53_CAM_1.csv\n",
            "  [52/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.43.45_CAM_1.csv\n",
            "  [53/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.45.20_CAM_1.csv\n",
            "  [54/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.16.59_CAM_1.csv\n",
            "  [55/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.09.55_CAM_1.csv\n",
            "  [56/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.46.08_CAM_1.csv\n",
            "  [57/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.17.46_CAM_1.csv\n",
            "  [58/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.10.45_CAM_1.csv\n",
            "  [59/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.15.49_CAM_1.csv\n",
            "  [60/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.18.37_CAM_1.csv\n",
            "  [61/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.11.37_CAM_1.csv\n",
            "  [62/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.16.51_CAM_1.csv\n",
            "  [63/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.19.24_CAM_1.csv\n",
            "  [64/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.12.29_CAM_1.csv\n",
            "  [65/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.18.00_CAM_1.csv\n",
            "  [66/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.20.17_CAM_1.csv\n",
            "  [67/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.13.19_CAM_1.csv\n",
            "  [68/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.19.05_CAM_1.csv\n",
            "  [69/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.22.53_CAM_1.csv\n",
            "  [70/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.22.43_CAM_1.csv\n",
            "  [71/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.19.49_CAM_1.csv\n",
            "  [72/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.26.00_CAM_1.csv\n",
            "  [73/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.20.45_CAM_1.csv\n",
            "  [74/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.23.38_CAM_1.csv\n",
            "  [75/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.13.16_CAM_1.csv\n",
            "  [76/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.21.38_CAM_1.csv\n",
            "  [77/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.14.22_CAM_1.csv\n",
            "  [78/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.24.32_CAM_1.csv\n",
            "  [79/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.07.30_CAM_1.csv\n",
            "  [80/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.15.20_CAM_1.csv\n",
            "  [81/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.25.26_CAM_1.csv\n",
            "  [82/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.16.13_CAM_1.csv\n",
            "  [83/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.08.20_CAM_1.csv\n",
            "  [84/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.26.20_CAM_1.csv\n",
            "  [85/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.09.19_CAM_1.csv\n",
            "  [86/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.17.17_CAM_1.csv\n",
            "  [87/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.27.16_CAM_1.csv\n",
            "  [88/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.18.38_CAM_1.csv\n",
            "  [89/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.10.25_CAM_1.csv\n",
            "  [90/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.52.51_CAM_1.csv\n",
            "  [91/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.11.17_CAM_1.csv\n",
            "  [92/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.19.28_CAM_1.csv\n",
            "  [93/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.54.00_CAM_1.csv\n",
            "  [94/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.12.07_CAM_1.csv\n",
            "  [95/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.01.06_CAM_1.csv\n",
            "  [96/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.12.55_CAM_1.csv\n",
            "  [97/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.01.55_CAM_1.csv\n",
            "  [98/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.54.55_CAM_1.csv\n",
            "  [99/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.13.47_CAM_1.csv\n",
            "  [100/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.02.45_CAM_1.csv\n",
            "  [101/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.55.52_CAM_1.csv\n",
            "  [102/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.03.39_CAM_1.csv\n",
            "  [103/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.04.44_CAM_1.csv\n",
            "  [104/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.20.51_CAM_1.csv\n",
            "  [105/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.21.48_CAM_1.csv\n",
            "  [106/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.15.09_CAM_1.csv\n",
            "  [107/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.16.15_CAM_1.csv\n",
            "  [108/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.17.09_CAM_1.csv\n",
            "  [109/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.18.05_CAM_1.csv\n",
            "  [110/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_15.11.39_CAM_1.csv\n",
            "  [111/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.04.49_CAM_1.csv\n",
            "  [112/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.05.59_CAM_1.csv\n",
            "  [113/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.07.10_CAM_1.csv\n",
            "  [114/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.08.25_CAM_1.csv\n",
            "  [115/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.10.55_CAM_1.csv\n",
            "  [116/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.12.05_CAM_1.csv\n",
            "  [117/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.13.11_CAM_1.csv\n",
            "--------------------------------------------------\n",
            "✅ 'Hundred' 안정성 분석 완료! (모든 기능 통합)\n",
            "저장된 파일: /content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis/keypoint_Hundred_Analysis_noseY_active/EDA_Hundred_Stability_Report_noseY_1119.csv\n",
            "총 117개의 'Hundred' 영상이 분석되었습니다.\n",
            "\n",
            "[결과 미리보기]\n",
            "   Leg_Stability_StdDev  Core_Stability_StdDev  Leg_Fatigue_Slope  \\\n",
            "0              7.145453              10.395685          -0.000987   \n",
            "1             11.625959              14.425257           0.012883   \n",
            "2             10.494114              14.326430           0.000686   \n",
            "3              5.149736               7.911254          -0.052365   \n",
            "4              4.165461               7.426217           0.000834   \n",
            "\n",
            "   Avg_Leg_Angle  Avg_Core_Angle  Active_Frame_Count  Active_Start_Frame  \\\n",
            "0       0.961365      169.213299               415.0                96.0   \n",
            "1       5.278925      162.819475               368.0               125.0   \n",
            "2       3.672617      165.262404               334.0               140.0   \n",
            "3       4.268073      163.161947               249.0               260.0   \n",
            "4       1.716612      169.680830               316.0               102.0   \n",
            "\n",
            "   Active_End_Frame                                        Source_File  \\\n",
            "0             510.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "1             492.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "2             473.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "3             508.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "4             419.0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...   \n",
            "\n",
            "   Person_ID  Level Place  \n",
            "0  actorP075  고급     A  \n",
            "1  actorP075  고급     A  \n",
            "2  actorP075  고급     A  \n",
            "3  actorP076  고급     A  \n",
            "4  actorP077  고급     A  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from scipy.stats import linregress\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. 드라이브 마운트 (최초 1회 실행)\n",
        "\n",
        "# --- 2. 분석 함수 정의 ---\n",
        "def calculate_hundred_metrics(df):\n",
        "    \"\"\"\n",
        "    Hundred 지표 (각도, 높이)를 계산\n",
        "    - (Smart Leg) 신뢰도 높은 다리 자동 선택\n",
        "    - (abs(dx)) 방향에 상관없이 예각(Angle) 계산\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # (1) 다리 선택 (잘 보이는 쪽으로)\n",
        "    # 원본 _conf 컬럼이 있는지 확인\n",
        "    if 'Left_Ankle_conf' not in df.columns or 'Right_Ankle_conf' not in df.columns:\n",
        "        print(\"    ⚠️ Ankle_conf 컬럼이 없어 '왼쪽 다리'를 기준으로 계산합니다.\")\n",
        "        use_left_leg_mask = True # conf 없으면 왼쪽 기준\n",
        "    else:\n",
        "        use_left_leg_mask = df['Left_Ankle_conf'] >= df['Right_Ankle_conf']\n",
        "\n",
        "    smart_ankle_x = np.where(use_left_leg_mask, df['norm_Left_Ankle_x'], df['norm_Right_Ankle_x'])\n",
        "    smart_ankle_y = np.where(use_left_leg_mask, df['norm_Left_Ankle_y'], df['norm_Right_Ankle_y'])\n",
        "    smart_hip_x = np.where(use_left_leg_mask, df['norm_Left_Hip_x'], df['norm_Right_Hip_x'])\n",
        "    smart_hip_y = np.where(use_left_leg_mask, df['norm_Left_Hip_y'], df['norm_Right_Hip_y'])\n",
        "\n",
        "    # (1_A) 다리 높이 각도 (Smart Leg 기준)\n",
        "    dy = -(smart_ankle_y - smart_hip_y) # Y축 반전\n",
        "    dx = np.abs(smart_ankle_x - smart_hip_x) # [수정] 절댓값 적용\n",
        "    df_out['Leg_Height_Angle'] = np.degrees(np.arctan2(dy, dx))\n",
        "\n",
        "    # (2) 코어 고정각 (Shoulder-Hip-Knee)\n",
        "    def get_angle(a, b, c):\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
        "\n",
        "    p_shoulder = df[['norm_Left_Shoulder_x', 'norm_Left_Shoulder_y']].values\n",
        "    p_hip = df[['norm_Left_Hip_x', 'norm_Left_Hip_y']].values\n",
        "    p_knee = df[['norm_Left_Knee_x', 'norm_Left_Knee_y']].values\n",
        "\n",
        "    df_out['Core_Angle'] = [get_angle(s, h, k) if not (np.isnan(s).any() or np.isnan(h).any() or np.isnan(k).any()) else np.nan\n",
        "                            for s, h, k in zip(p_shoulder, p_hip, p_knee)]\n",
        "\n",
        "    # (3) 상체 컬업 (Nose가 Shoulder Center보다 얼마나 위에 있나)\n",
        "    shoulder_center_y = (df['norm_Left_Shoulder_y'] + df['norm_Right_Shoulder_y']) / 2\n",
        "    df_out['Head_Curl_Position'] = shoulder_center_y - df['norm_Nose_y']\n",
        "\n",
        "    return df_out\n",
        "\n",
        "def get_dominant_angle(series, bin_size=1.0):\n",
        "    \"\"\"\n",
        "    연속된 각도 데이터에서 '가장 오랫동안 유지된 각도(최빈값)'를 찾습니다.\n",
        "    bin_size: 각도를 뭉뚱그릴 단위 (1.0도 단위로 반올림해서 셈)\n",
        "    \"\"\"\n",
        "    if len(series) == 0: return np.nan\n",
        "\n",
        "    # 1. 각도를 정수(또는 bin_size) 단위로 반올림 (노이즈 제거)\n",
        "    rounded_series = (series / bin_size).round() * bin_size\n",
        "\n",
        "    # 2. 가장 많이 등장한 값(Mode) 찾기\n",
        "    mode_value = rounded_series.mode()\n",
        "\n",
        "    if len(mode_value) > 0:\n",
        "        return mode_value[0] # 최빈값이 여러 개면 첫 번째 것 선택\n",
        "    return np.nan\n",
        "\n",
        "def analyze_stability(group):\n",
        "    \"\"\"\n",
        "    [최신 수정]\n",
        "    1. Nose_Y로 Active 구간 자르기\n",
        "    2. '평균' 대신 '가장 오래 유지한 각도(Dominant Angle)' 추출\n",
        "    \"\"\"\n",
        "\n",
        "    # --- [1. 보간(Interpolation) 전처리] ---\n",
        "    parts_to_clean = ['Left_Ankle', 'Right_Ankle', 'Left_Hip', 'Right_Hip',\n",
        "                      'Left_Shoulder', 'Right_Shoulder', 'Nose', 'Left_Knee']\n",
        "    group_clean = group.copy()\n",
        "    for part in parts_to_clean:\n",
        "        if f'{part}_conf' in group_clean.columns:\n",
        "            low_conf_mask = group_clean[f'{part}_conf'] < 0.5\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_x'] = np.nan\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_y'] = np.nan\n",
        "    norm_cols = [col for col in group_clean.columns if col.startswith('norm_')]\n",
        "    group_clean[norm_cols] = group_clean[norm_cols].interpolate(method='linear', limit_direction='both', limit=5)\n",
        "\n",
        "    # 1. 지표 계산 (몸통 기준 각도 함수 사용)\n",
        "    group_with_metrics = calculate_hundred_metrics(group_clean)\n",
        "\n",
        "    # --- [2. Active 구간 자르기 (Nose_Y 로직)] ---\n",
        "    min_nose_y = group_with_metrics['norm_Nose_y'].min()\n",
        "    max_nose_y = group_with_metrics['norm_Nose_y'].max()\n",
        "    threshold = max_nose_y - (max_nose_y - min_nose_y) * 0.4 # 하위 40%\n",
        "\n",
        "    active_frames = group_with_metrics[group_with_metrics['norm_Nose_y'] > threshold].copy()\n",
        "\n",
        "    if active_frames.empty: return None\n",
        "\n",
        "    # 가장 긴 연속 구간 선택\n",
        "    active_frames['frame_diff'] = active_frames['Frame'].diff()\n",
        "    active_frames['block_id'] = (active_frames['frame_diff'] > 5).cumsum()\n",
        "    longest_block_id = active_frames.groupby('block_id').size().idxmax()\n",
        "    final_active_frames = active_frames[active_frames['block_id'] == longest_block_id]\n",
        "\n",
        "    if len(final_active_frames) < 30: return None # 1초 미만 무시\n",
        "\n",
        "    # --- [3. 핵심 수정: Dominant Angle & 유지율 계산] ---\n",
        "\n",
        "    # (1) Dominant Angle (가장 오래 유지한 각도)\n",
        "    dominant_leg_angle = get_dominant_angle(final_active_frames['Leg_Height_Angle'])\n",
        "    dominant_core_angle = get_dominant_angle(final_active_frames['Core_Angle'])\n",
        "\n",
        "    # (2) 유지율 (Consistency Score)\n",
        "    # Dominant Angle에서 ±5도 이내에 들어온 프레임 비율\n",
        "    leg_error_margin = 5.0\n",
        "    frames_in_zone = final_active_frames[\n",
        "        (final_active_frames['Leg_Height_Angle'] >= dominant_leg_angle - leg_error_margin) &\n",
        "        (final_active_frames['Leg_Height_Angle'] <= dominant_leg_angle + leg_error_margin)\n",
        "    ]\n",
        "    consistency_score = (len(frames_in_zone) / len(final_active_frames)) * 100 # 퍼센트(%)\n",
        "\n",
        "    # (3) 기존 통계량\n",
        "    stability_score = final_active_frames['Leg_Height_Angle'].std() # 흔들림\n",
        "\n",
        "    if len(final_active_frames['Frame'].unique()) > 1:\n",
        "        slope, _ = np.polyfit(final_active_frames['Frame'], final_active_frames['Leg_Height_Angle'], 1)\n",
        "    else:\n",
        "        slope = 0\n",
        "\n",
        "    start_frame = final_active_frames['Frame'].min()\n",
        "    end_frame = final_active_frames['Frame'].max()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Dominant_Leg_Angle': dominant_leg_angle,   # [New] 사용자가 의도한 각도\n",
        "        'Dominant_Core_Angle': dominant_core_angle, # [New] 사용자가 의도한 코어각\n",
        "        'Consistency_Score': consistency_score,     # [New] 유지율 (%)\n",
        "        'Leg_Stability_StdDev': stability_score,    # 흔들림\n",
        "        'Leg_Fatigue_Slope': slope,                 # 처짐 기울기\n",
        "        'Active_Frame_Count': len(final_active_frames),\n",
        "        'Active_Start_Frame': start_frame,\n",
        "        'Active_End_Frame': end_frame\n",
        "    })\n",
        "\n",
        "\n",
        "# --- 3. 메인 실행부 (Loop 방식) ---\n",
        "\n",
        "# [경로 설정]\n",
        "drive_root = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/dataset/Hundred'\n",
        "norm_folder_path = os.path.join(drive_root, 'norm')\n",
        "all_files = glob.glob(os.path.join(norm_folder_path, \"norm_*.csv\"))\n",
        "\n",
        "output_path = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis'\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"❌ '{norm_folder_path}' 경로에 'norm_*.csv' 파일이 없습니다. 경로를 확인해주세요.\")\n",
        "else:\n",
        "    print(f\"✅ 총 {len(all_files)}개의 정규화된 CSV 파일을 분석합니다.\")\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for i, file_path in enumerate(all_files):\n",
        "        filename = os.path.basename(file_path)\n",
        "        print(f\"  [{i+1}/{len(all_files)}] 분석 중: {filename}\")\n",
        "\n",
        "        try:\n",
        "            # 1. 파일명에서 메타데이터(ID, Level) 추출 (안전한 방식으로 수정)\n",
        "            clean_name = filename.replace('norm_', '').replace('.csv', '')\n",
        "            parts = clean_name.split('_')\n",
        "\n",
        "            place = parts[2]\n",
        "            level = parts[5]\n",
        "            person_id = parts[6]\n",
        "            pose_name = parts[4]\n",
        "\n",
        "            # 키워드로 검색 (순서가 바뀌어도 OK)\n",
        "            for part in parts:\n",
        "                if part.startswith('actor'):\n",
        "                    person_id = part\n",
        "                if part in ['고급', '중급', '초급']:\n",
        "                    level = part\n",
        "                if part.lower() == 'hundred':\n",
        "                    pose_name = 'Hundred'\n",
        "\n",
        "            # 'Hundred' 파일이 아니면 건너뛰기\n",
        "            if pose_name != 'Hundred':\n",
        "                print(f\"    ... [Skip] 'Hundred' 파일이 아니므로 건너뜁니다.\")\n",
        "                continue\n",
        "\n",
        "            # 2. CSV 읽기\n",
        "            df_single_file = pd.read_csv(file_path)\n",
        "\n",
        "            # 3. '안정성' 분석 (보간 + 지표 계산)\n",
        "            # (calculate_hundred_metrics는 analyze_stability 내부에서 호출됨)\n",
        "            stability_summary = analyze_stability(df_single_file)\n",
        "\n",
        "            # 4. 결과 저장\n",
        "            if stability_summary is not None:\n",
        "                stability_summary['Source_File'] = filename\n",
        "                stability_summary['Person_ID'] = person_id\n",
        "                stability_summary['Level'] = level\n",
        "                stability_summary['Place'] = place\n",
        "                results_list.append(stability_summary)\n",
        "\n",
        "        except KeyError as e:\n",
        "            print(f\"    ⚠️ {filename} 처리 중 오류 발생 (KeyError): {e} - CSV에 필요한 컬럼이 없습니다.\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ {filename} 처리 중 알 수 없는 오류 발생: {e}\")\n",
        "\n",
        "    # --- 4. 최종 결과 저장 (오류 수정된 버전) ---\n",
        "    if results_list:\n",
        "        df_final_eda = pd.DataFrame(results_list)\n",
        "\n",
        "        output_dir = os.path.join(output_path, \"keypoint_Hundred_Analysis\") # 새 폴더 이름\n",
        "        save_path = os.path.join(output_dir, \"EDA_Hundred_Stability_Report_mode_1119.csv\")\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        df_final_eda.to_csv(save_path, index=False)\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\"✅ 'Hundred' 안정성 분석 완료! (모든 기능 통합)\")\n",
        "        print(f\"저장된 파일: {save_path}\")\n",
        "        print(f\"총 {len(df_final_eda)}개의 'Hundred' 영상이 분석되었습니다.\")\n",
        "\n",
        "        print(\"\\n[결과 미리보기]\")\n",
        "        print(df_final_eda.head())\n",
        "    else:\n",
        "        print(\"분석할 'Hundred' 영상이 없거나 유효한 데이터가 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9APKfMpv_9I",
        "outputId": "e3d0de7b-76dd-449a-cdb3-454d28932b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 총 117개의 정규화된 CSV 파일을 분석합니다.\n",
            "  [1/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.14.40_CAM_1.csv\n",
            "  [2/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.15.35_CAM_1.csv\n",
            "  [3/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP075_20220930_11.17.02_CAM_1.csv\n",
            "  [4/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP076_20220929_15.26.04_CAM_1.csv\n",
            "  [5/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.10.42_CAM_1.csv\n",
            "  [6/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.11.30_CAM_1.csv\n",
            "  [7/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP077_20220929_11.12.19_CAM_1.csv\n",
            "  [8/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.01.17_CAM_1.csv\n",
            "  [9/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.02.15_CAM_1.csv\n",
            "  [10/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP079_20220928_12.03.10_CAM_1.csv\n",
            "  [11/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.50.10_CAM_1.csv\n",
            "  [12/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.51.01_CAM_1.csv\n",
            "  [13/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.51.54_CAM_1.csv\n",
            "  [14/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.52.49_CAM_1.csv\n",
            "  [15/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221006_15.53.42_CAM_1.csv\n",
            "  [16/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.35.27_CAM_1.csv\n",
            "  [17/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.36.16_CAM_1.csv\n",
            "  [18/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.37.05_CAM_1.csv\n",
            "  [19/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.37.52_CAM_1.csv\n",
            "  [20/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.38.46_CAM_1.csv\n",
            "  [21/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP091_20221011_11.39.39_CAM_1.csv\n",
            "  [22/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP092_20221005_11.17.31_CAM_1.csv\n",
            "  [23/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.36.07_CAM_1.csv\n",
            "  [24/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.36.53_CAM_1.csv\n",
            "  [25/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.37.42_CAM_1.csv\n",
            "  [26/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.38.28_CAM_1.csv\n",
            "  [27/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.39.18_CAM_1.csv\n",
            "  [28/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.40.12_CAM_1.csv\n",
            "  [29/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.40.59_CAM_1.csv\n",
            "  [30/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.41.49_CAM_1.csv\n",
            "  [31/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.42.35_CAM_1.csv\n",
            "  [32/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP069_20221017_11.43.23_CAM_1.csv\n",
            "  [33/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.36.09_CAM_1.csv\n",
            "  [34/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.37.03_CAM_1.csv\n",
            "  [35/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.37.57_CAM_1.csv\n",
            "  [36/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.38.54_CAM_1.csv\n",
            "  [37/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.30.00_CAM_1.csv\n",
            "  [38/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.40.24_CAM_1.csv\n",
            "  [39/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.39.44_CAM_1.csv\n",
            "  [40/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.30.46_CAM_1.csv\n",
            "  [41/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.41.26_CAM_1.csv\n",
            "  [42/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP098_20221007_11.31.33_CAM_1.csv\n",
            "  [43/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.40.59_CAM_1.csv\n",
            "  [44/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.42.27_CAM_1.csv\n",
            "  [45/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.07.14_CAM_1.csv\n",
            "  [46/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.42.02_CAM_1.csv\n",
            "  [47/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.43.26_CAM_1.csv\n",
            "  [48/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.08.04_CAM_1.csv\n",
            "  [49/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.42.56_CAM_1.csv\n",
            "  [50/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.44.29_CAM_1.csv\n",
            "  [51/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.08.53_CAM_1.csv\n",
            "  [52/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP086_20221012_15.43.45_CAM_1.csv\n",
            "  [53/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.45.20_CAM_1.csv\n",
            "  [54/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.16.59_CAM_1.csv\n",
            "  [55/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.09.55_CAM_1.csv\n",
            "  [56/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP063_20221014_15.46.08_CAM_1.csv\n",
            "  [57/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.17.46_CAM_1.csv\n",
            "  [58/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.10.45_CAM_1.csv\n",
            "  [59/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.15.49_CAM_1.csv\n",
            "  [60/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.18.37_CAM_1.csv\n",
            "  [61/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.11.37_CAM_1.csv\n",
            "  [62/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.16.51_CAM_1.csv\n",
            "  [63/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.19.24_CAM_1.csv\n",
            "  [64/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.12.29_CAM_1.csv\n",
            "  [65/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.18.00_CAM_1.csv\n",
            "  [66/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.20.17_CAM_1.csv\n",
            "  [67/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP096_20221020_11.13.19_CAM_1.csv\n",
            "  [68/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.19.05_CAM_1.csv\n",
            "  [69/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.22.53_CAM_1.csv\n",
            "  [70/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.22.43_CAM_1.csv\n",
            "  [71/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.19.49_CAM_1.csv\n",
            "  [72/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221014_11.26.00_CAM_1.csv\n",
            "  [73/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.20.45_CAM_1.csv\n",
            "  [74/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.23.38_CAM_1.csv\n",
            "  [75/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.13.16_CAM_1.csv\n",
            "  [76/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP064_20221020_15.21.38_CAM_1.csv\n",
            "  [77/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.14.22_CAM_1.csv\n",
            "  [78/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.24.32_CAM_1.csv\n",
            "  [79/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.07.30_CAM_1.csv\n",
            "  [80/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.15.20_CAM_1.csv\n",
            "  [81/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.25.26_CAM_1.csv\n",
            "  [82/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.16.13_CAM_1.csv\n",
            "  [83/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.08.20_CAM_1.csv\n",
            "  [84/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.26.20_CAM_1.csv\n",
            "  [85/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.09.19_CAM_1.csv\n",
            "  [86/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.17.17_CAM_1.csv\n",
            "  [87/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.27.16_CAM_1.csv\n",
            "  [88/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.18.38_CAM_1.csv\n",
            "  [89/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.10.25_CAM_1.csv\n",
            "  [90/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.52.51_CAM_1.csv\n",
            "  [91/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.11.17_CAM_1.csv\n",
            "  [92/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP082_20221027_14.19.28_CAM_1.csv\n",
            "  [93/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.54.00_CAM_1.csv\n",
            "  [94/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.12.07_CAM_1.csv\n",
            "  [95/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.01.06_CAM_1.csv\n",
            "  [96/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.12.55_CAM_1.csv\n",
            "  [97/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.01.55_CAM_1.csv\n",
            "  [98/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.54.55_CAM_1.csv\n",
            "  [99/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP067_20221012_11.13.47_CAM_1.csv\n",
            "  [100/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.02.45_CAM_1.csv\n",
            "  [101/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP099_20221028_10.55.52_CAM_1.csv\n",
            "  [102/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.03.39_CAM_1.csv\n",
            "  [103/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP093_20221025_14.04.44_CAM_1.csv\n",
            "  [104/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.20.51_CAM_1.csv\n",
            "  [105/117] 분석 중: norm_필라테스_가산_A_Mat_Hundred_고급_actorP095_20221024_14.21.48_CAM_1.csv\n",
            "  [106/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.15.09_CAM_1.csv\n",
            "  [107/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.16.15_CAM_1.csv\n",
            "  [108/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.17.09_CAM_1.csv\n",
            "  [109/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_14.18.05_CAM_1.csv\n",
            "  [110/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP079_20221028_15.11.39_CAM_1.csv\n",
            "  [111/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.04.49_CAM_1.csv\n",
            "  [112/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.05.59_CAM_1.csv\n",
            "  [113/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.07.10_CAM_1.csv\n",
            "  [114/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.08.25_CAM_1.csv\n",
            "  [115/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.10.55_CAM_1.csv\n",
            "  [116/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.12.05_CAM_1.csv\n",
            "  [117/117] 분석 중: norm_필라테스_가산_C_Mat_Hundred_고급_actorP080_20221024_11.13.11_CAM_1.csv\n",
            "--------------------------------------------------\n",
            "✅ 'Hundred' 안정성 분석 완료! (모든 기능 통합)\n",
            "저장된 파일: /content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis/keypoint_Hundred_Analysis/EDA_Hundred_Stability_Report_mode_1119.csv\n",
            "총 117개의 'Hundred' 영상이 분석되었습니다.\n",
            "\n",
            "[결과 미리보기]\n",
            "   Dominant_Leg_Angle  Dominant_Core_Angle  Consistency_Score  \\\n",
            "0                -4.0                178.0          61.445783   \n",
            "1                -6.0                176.0          53.260870   \n",
            "2                -5.0                179.0          55.988024   \n",
            "3                 7.0                159.0          75.502008   \n",
            "4                 5.0                178.0          56.962025   \n",
            "\n",
            "   Leg_Stability_StdDev  Leg_Fatigue_Slope  Active_Frame_Count  \\\n",
            "0              7.145453          -0.000987               415.0   \n",
            "1             11.625959           0.012883               368.0   \n",
            "2             10.494114           0.000686               334.0   \n",
            "3              5.149736          -0.052365               249.0   \n",
            "4              4.165461           0.000834               316.0   \n",
            "\n",
            "   Active_Start_Frame  Active_End_Frame  \\\n",
            "0                96.0             510.0   \n",
            "1               125.0             492.0   \n",
            "2               140.0             473.0   \n",
            "3               260.0             508.0   \n",
            "4               102.0             419.0   \n",
            "\n",
            "                                         Source_File  Person_ID  Level Place  \n",
            "0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP075  고급     A  \n",
            "1  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP075  고급     A  \n",
            "2  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP075  고급     A  \n",
            "3  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP076  고급     A  \n",
            "4  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP077  고급     A  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import glob\n",
        "import os\n",
        "from scipy.stats import linregress\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. 드라이브 마운트\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# --- [헬퍼 함수] 최빈값 계산 ---\n",
        "def get_dominant_angle(series, bin_size=1.0):\n",
        "    \"\"\"\n",
        "    가장 오랫동안 유지된 각도(최빈값)를 찾습니다.\n",
        "    \"\"\"\n",
        "    if len(series) == 0: return np.nan\n",
        "    # 각도를 정수(bin_size) 단위로 반올림하여 노이즈 제거 후 최빈값 찾기\n",
        "    rounded_series = (series / bin_size).round() * bin_size\n",
        "    mode_value = rounded_series.mode()\n",
        "\n",
        "    if len(mode_value) > 0:\n",
        "        return mode_value[0]\n",
        "    return np.nan\n",
        "\n",
        "# --- [헬퍼 함수] 몸통 기준 각도 계산 ---\n",
        "def get_relative_lift_angle(neck, hip, ankle):\n",
        "    vec_torso = neck - hip\n",
        "    vec_leg = ankle - hip\n",
        "\n",
        "    dot_product = np.dot(vec_torso, vec_leg)\n",
        "    norm_torso = np.linalg.norm(vec_torso)\n",
        "    norm_leg = np.linalg.norm(vec_leg)\n",
        "\n",
        "    if norm_torso == 0 or norm_leg == 0: return 0\n",
        "\n",
        "    cosine_angle = dot_product / (norm_torso * norm_leg)\n",
        "    angle_rad = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
        "    angle_deg = np.degrees(angle_rad)\n",
        "\n",
        "    return 180 - angle_deg\n",
        "\n",
        "# --- 2. 지표 계산 함수 ---\n",
        "def calculate_hundred_metrics(df):\n",
        "    df_out = df.copy()\n",
        "\n",
        "    # (1) 다리 선택\n",
        "    if 'Left_Ankle_conf' not in df.columns or 'Right_Ankle_conf' not in df.columns:\n",
        "        use_left_leg_mask = True\n",
        "    else:\n",
        "        use_left_leg_mask = df['Left_Ankle_conf'] >= df['Right_Ankle_conf']\n",
        "\n",
        "    smart_ankle_x = np.where(use_left_leg_mask, df['norm_Left_Ankle_x'], df['norm_Right_Ankle_x'])\n",
        "    smart_ankle_y = np.where(use_left_leg_mask, df['norm_Left_Ankle_y'], df['norm_Right_Ankle_y'])\n",
        "    smart_hip_x = np.where(use_left_leg_mask, df['norm_Left_Hip_x'], df['norm_Right_Hip_x'])\n",
        "    smart_hip_y = np.where(use_left_leg_mask, df['norm_Left_Hip_y'], df['norm_Right_Hip_y'])\n",
        "\n",
        "    # (1_A) 다리 높이 각도 (몸통 기준 Relative Angle)\n",
        "    shoulder_center_x = (df['norm_Left_Shoulder_x'] + df['norm_Right_Shoulder_x']) / 2\n",
        "    shoulder_center_y = (df['norm_Left_Shoulder_y'] + df['norm_Right_Shoulder_y']) / 2\n",
        "\n",
        "    neck_coords = np.array([shoulder_center_x, shoulder_center_y]).T\n",
        "    smart_hip_coords = np.array([smart_hip_x, smart_hip_y]).T\n",
        "    smart_ankle_coords = np.array([smart_ankle_x, smart_ankle_y]).T\n",
        "\n",
        "    angle_list = []\n",
        "    for i in range(len(df)):\n",
        "        angle_val = get_relative_lift_angle(neck_coords[i], smart_hip_coords[i], smart_ankle_coords[i])\n",
        "        angle_list.append(angle_val)\n",
        "\n",
        "    df_out['Leg_Height_Angle'] = angle_list\n",
        "\n",
        "    # (2) 코어 고정각\n",
        "    def get_angle(a, b, c):\n",
        "        ba = a - b\n",
        "        bc = c - b\n",
        "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
        "        return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
        "\n",
        "    p_shoulder = df[['norm_Left_Shoulder_x', 'norm_Left_Shoulder_y']].values\n",
        "    p_hip = df[['norm_Left_Hip_x', 'norm_Left_Hip_y']].values\n",
        "    p_knee = df[['norm_Left_Knee_x', 'norm_Left_Knee_y']].values\n",
        "\n",
        "    df_out['Core_Angle'] = [get_angle(s, h, k) if not (np.isnan(s).any() or np.isnan(h).any() or np.isnan(k).any()) else np.nan\n",
        "                            for s, h, k in zip(p_shoulder, p_hip, p_knee)]\n",
        "\n",
        "    # (3) 상체 컬업\n",
        "    df_out['Head_Curl_Position'] = shoulder_center_y - df['norm_Nose_y']\n",
        "\n",
        "    return df_out\n",
        "\n",
        "def analyze_stability(group):\n",
        "    \"\"\"\n",
        "    [수정됨] Nose_Y 로직 폐기 -> 다리 각도(>5도) 기준으로 Active 구간 선정\n",
        "    \"\"\"\n",
        "\n",
        "    # --- [1. 보간(Interpolation)] ---\n",
        "    parts_to_clean = ['Left_Ankle', 'Right_Ankle', 'Left_Hip', 'Right_Hip',\n",
        "                      'Left_Shoulder', 'Right_Shoulder', 'Nose', 'Left_Knee']\n",
        "    group_clean = group.copy()\n",
        "    for part in parts_to_clean:\n",
        "        if f'{part}_conf' in group_clean.columns:\n",
        "            low_conf_mask = group_clean[f'{part}_conf'] < 0.5\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_x'] = np.nan\n",
        "            group_clean.loc[low_conf_mask, f'norm_{part}_y'] = np.nan\n",
        "    norm_cols = [col for col in group_clean.columns if col.startswith('norm_')]\n",
        "    group_clean[norm_cols] = group_clean[norm_cols].interpolate(method='linear', limit_direction='both', limit=5)\n",
        "\n",
        "    # 1. 지표 계산\n",
        "    group_with_metrics = calculate_hundred_metrics(group_clean)\n",
        "\n",
        "    # --- [2. Active 구간 선정 (심플 로직)] ---\n",
        "    # 다리가 엉덩이보다 높게(각도 > 5도) 올라간 구간만 선택\n",
        "    # (0도 대신 5도를 쓰는 이유: 바닥에 누워있을 때의 미세한 떨림/노이즈 제외)\n",
        "    active_frames = group_with_metrics[group_with_metrics['Leg_Height_Angle'] > 5].copy()\n",
        "\n",
        "    if active_frames.empty: return None\n",
        "\n",
        "    # 연속성 체크: 가장 긴 구간 선택 (잠깐 든 것 제외)\n",
        "    active_frames['frame_diff'] = active_frames['Frame'].diff()\n",
        "    active_frames['block_id'] = (active_frames['frame_diff'] > 5).cumsum()\n",
        "    longest_block_id = active_frames.groupby('block_id').size().idxmax()\n",
        "    final_active_frames = active_frames[active_frames['block_id'] == longest_block_id]\n",
        "\n",
        "    if len(final_active_frames) < 30: return None # 1초 미만 무시\n",
        "\n",
        "    # --- [3. 통계 산출 (최빈값 포함)] ---\n",
        "\n",
        "    # (1) Dominant Angle (가장 오래 유지한 각도)\n",
        "    dominant_leg_angle = get_dominant_angle(final_active_frames['Leg_Height_Angle'])\n",
        "    dominant_core_angle = get_dominant_angle(final_active_frames['Core_Angle'])\n",
        "\n",
        "    # (2) 유지율 (Consistency Score)\n",
        "    leg_error_margin = 5.0\n",
        "    frames_in_zone = final_active_frames[\n",
        "        (final_active_frames['Leg_Height_Angle'] >= dominant_leg_angle - leg_error_margin) &\n",
        "        (final_active_frames['Leg_Height_Angle'] <= dominant_leg_angle + leg_error_margin)\n",
        "    ]\n",
        "    consistency_score = (len(frames_in_zone) / len(final_active_frames)) * 100\n",
        "\n",
        "    # (3) 기존 지표\n",
        "    stability_score = final_active_frames['Leg_Height_Angle'].std()\n",
        "\n",
        "    if len(final_active_frames['Frame'].unique()) > 1:\n",
        "        slope, _ = np.polyfit(final_active_frames['Frame'], final_active_frames['Leg_Height_Angle'], 1)\n",
        "    else:\n",
        "        slope = 0\n",
        "\n",
        "    start_frame = final_active_frames['Frame'].min()\n",
        "    end_frame = final_active_frames['Frame'].max()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Dominant_Leg_Angle': dominant_leg_angle,   # [핵심] 사용자가 의도한 각도 (최빈값)\n",
        "        'Dominant_Core_Angle': dominant_core_angle,\n",
        "        'Consistency_Score': consistency_score,     # 유지율\n",
        "        'Leg_Stability_StdDev': stability_score,\n",
        "        'Leg_Fatigue_Slope': slope,\n",
        "        'Active_Frame_Count': len(final_active_frames),\n",
        "        'Active_Start_Frame': start_frame,\n",
        "        'Active_End_Frame': end_frame\n",
        "    })\n",
        "\n",
        "# --- 3. 메인 실행부 ---\n",
        "\n",
        "drive_root = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/dataset/Hundred'\n",
        "norm_folder_path = os.path.join(drive_root,'keypoint_Hundred',  'norm_x')\n",
        "all_files = glob.glob(os.path.join(norm_folder_path, \"norm_*.csv\"))\n",
        "\n",
        "output_path = '/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis'\n",
        "\n",
        "if not all_files:\n",
        "    print(f\"❌ '{norm_folder_path}' 경로에 'norm_*.csv' 파일이 없습니다.\")\n",
        "else:\n",
        "    print(f\"✅ 총 {len(all_files)}개의 파일을 분석합니다 (다리 각도 > 5도 기준).\")\n",
        "\n",
        "    results_list = []\n",
        "\n",
        "    for i, file_path in enumerate(all_files):\n",
        "        filename = os.path.basename(file_path)\n",
        "        if i % 10 == 0: print(f\"  [{i+1}/{len(all_files)}] 처리 중...\")\n",
        "\n",
        "        try:\n",
        "            clean_name = filename.replace('norm_', '').replace('.csv', '')\n",
        "            parts = clean_name.split('_')\n",
        "\n",
        "            place = parts[2]\n",
        "            level = parts[5]\n",
        "            person_id = parts[6]\n",
        "            pose_name = parts[4]\n",
        "\n",
        "            for part in parts:\n",
        "                if part.startswith('actor'): person_id = part\n",
        "                if part in ['고급', '중급', '초급']: level = part\n",
        "                if part.lower() == 'hundred': pose_name = 'Hundred'\n",
        "\n",
        "            if pose_name != 'Hundred': continue\n",
        "\n",
        "            df_single_file = pd.read_csv(file_path)\n",
        "            stability_summary = analyze_stability(df_single_file)\n",
        "\n",
        "            if stability_summary is not None:\n",
        "                stability_summary['Source_File'] = filename\n",
        "                stability_summary['Person_ID'] = person_id\n",
        "                stability_summary['Level'] = level\n",
        "                stability_summary['Place'] = place\n",
        "                results_list.append(stability_summary)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"    ⚠️ {filename} 오류: {e}\")\n",
        "\n",
        "    if results_list:\n",
        "        df_final_eda = pd.DataFrame(results_list)\n",
        "\n",
        "        output_dir = os.path.join(output_path, \"keypoint_Hundred_Analysis\")\n",
        "        save_path = os.path.join(output_dir, \"EDA_Hundred_Angle_Mode_Report_1124.csv\") # 파일명 변경\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        df_final_eda.to_csv(save_path, index=False)\n",
        "\n",
        "        print(\"-\" * 50)\n",
        "        print(\"✅ 분석 완료! (다리 각도 기반 최빈값 분석)\")\n",
        "        print(f\"저장된 파일: {save_path}\")\n",
        "        print(df_final_eda.head())\n",
        "    else:\n",
        "        print(\"유효한 데이터가 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6ha9RqCxX77",
        "outputId": "b0aa8bc7-fd03-431f-ae2e-b43522a6f0dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "✅ 총 117개의 파일을 분석합니다 (다리 각도 > 5도 기준).\n",
            "  [1/117] 처리 중...\n",
            "  [11/117] 처리 중...\n",
            "  [21/117] 처리 중...\n",
            "  [31/117] 처리 중...\n",
            "  [41/117] 처리 중...\n",
            "  [51/117] 처리 중...\n",
            "  [61/117] 처리 중...\n",
            "  [71/117] 처리 중...\n",
            "  [81/117] 처리 중...\n",
            "  [91/117] 처리 중...\n",
            "  [101/117] 처리 중...\n",
            "  [111/117] 처리 중...\n",
            "--------------------------------------------------\n",
            "✅ 분석 완료! (다리 각도 기반 최빈값 분석)\n",
            "저장된 파일: /content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis/keypoint_Hundred_Analysis/EDA_Hundred_Angle_Mode_Report_1124.csv\n",
            "   Dominant_Leg_Angle  Dominant_Core_Angle  Consistency_Score  \\\n",
            "0                16.0                165.0          60.000000   \n",
            "1                27.0                149.0          62.983425   \n",
            "2                25.0                151.0          65.359477   \n",
            "3                23.0                159.0          88.181818   \n",
            "4                14.0                168.0          58.252427   \n",
            "\n",
            "   Leg_Stability_StdDev  Leg_Fatigue_Slope  Active_Frame_Count  \\\n",
            "0              6.235150          -0.011494               180.0   \n",
            "1              7.492381          -0.017038               181.0   \n",
            "2              7.097681          -0.040306               153.0   \n",
            "3              4.770224          -0.006653               220.0   \n",
            "4              5.002649          -0.033483               206.0   \n",
            "\n",
            "   Active_Start_Frame  Active_End_Frame  \\\n",
            "0               207.0             386.0   \n",
            "1               226.0             406.0   \n",
            "2               231.0             383.0   \n",
            "3               232.0             457.0   \n",
            "4               157.0             362.0   \n",
            "\n",
            "                                         Source_File  Person_ID  Level Place  \n",
            "0  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP075  고급     A  \n",
            "1  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP075  고급     A  \n",
            "2  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP075  고급     A  \n",
            "3  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP076  고급     A  \n",
            "4  norm_필라테스_가산_A_Mat_Hundred_고급_actor...  actorP077  고급     A  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# 1. CSV 파일 불러오기 (파일명은 실제 파일로 변경하세요)\n",
        "df = pd.read_csv('/content/drive/MyDrive/shared_googledrive(Sessac Final Project)/analysis/keypoint_Hundred_Analysis/EDA_Hundred_Angle_Mode_Report_1124.csv')\n",
        "df['Leg_Angle'] = df['Dominant_Leg_Angle']\n",
        "\n",
        "print(f\"--- [1] 전체 원본 데이터 ({len(df)}개) ---\")\n",
        "desc = df['Leg_Angle'].describe()\n",
        "Q1 = desc['25%']\n",
        "Q3 = desc['75%']\n",
        "IQR = Q3 - Q1\n",
        "Lower_Fence = Q1 - 1.5 * IQR\n",
        "Upper_Fence = Q3 + 1.5 * IQR\n",
        "\n",
        "print(f\"Q1: {Q1:.2f} | Q3: {Q3:.2f}\")\n",
        "print(f\"IQR 범위 (Boxplot): {Lower_Fence:.2f} ~ {Upper_Fence:.2f}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 방법 A: 도메인 지식 기반 제거 (물리적 필터링)\n",
        "# 논리: \"5도 미만은 바닥에 닿은 쉬는 시간이고, 80도 초과는 측정이 잘못된 것이다.\"\n",
        "# ---------------------------------------------------------\n",
        "filtered_df_A = df[(df['Leg_Angle'] >= 10) & (df['Leg_Angle'] <= 70)]\n",
        "\n",
        "print(f\"\\n--- [2] 물리적 노이즈(10도 미만, 70도 초과) 제거 후 ({len(filtered_df_A)}개) ---\")\n",
        "desc_A = filtered_df_A['Leg_Angle'].describe()\n",
        "Q1_A = desc_A['25%']\n",
        "Q3_A = desc_A['75%']\n",
        "IQR_A = Q3_A - Q1_A\n",
        "Lower_A = Q1_A - 1.5 * IQR_A\n",
        "Upper_A = Q3_A + 1.5 * IQR_A\n",
        "\n",
        "print(f\"재계산된 Q1: {Q1_A:.2f} | Q3: {Q3_A:.2f}\")\n",
        "print(f\"재계산된 상/하한: {Lower_A:.2f} ~ {Upper_A:.2f}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 방법 B: 통계적 절사 (Trimming) - 상위/하위 10% 제거\n",
        "# 논리: \"양쪽 끝 10%의 극단적인 수행을 제외하고, '보편적인 수행'만 보겠다.\"\n",
        "# ---------------------------------------------------------\n",
        "q_low = df['Leg_Angle'].quantile(0.10) # 하위 10% 지점\n",
        "q_high = df['Leg_Angle'].quantile(0.90) # 상위 10% 지점\n",
        "\n",
        "filtered_df_B = df[(df['Leg_Angle'] >= q_low) & (df['Leg_Angle'] <= q_high)]\n",
        "\n",
        "print(f\"\\n--- [3] 양쪽 끝 10% 절사(Trimming) 후 ({len(filtered_df_B)}개) ---\")\n",
        "desc_B = filtered_df_B['Leg_Angle'].describe()\n",
        "Q1_B = desc_B['25%']\n",
        "Q3_B = desc_B['75%']\n",
        "IQR_B = Q3_B - Q1_B\n",
        "Lower_B = Q1_B - 1.5 * IQR_B\n",
        "Upper_B = Q3_B + 1.5 * IQR_B\n",
        "\n",
        "print(f\"절사 기준: {q_low:.2f}도 ~ {q_high:.2f}도 사이만 남김\")\n",
        "print(f\"재계산된 Q1: {Q1_B:.2f} | Q3: {Q3_B:.2f}\")\n",
        "print(f\"재계산된 상/하한: {Lower_B:.2f} ~ {Upper_B:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqRp3gNfaekJ",
        "outputId": "eb2e74cb-53a4-47f0-b908-9d83dc8427c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- [1] 전체 원본 데이터 (117개) ---\n",
            "Q1: 18.00 | Q3: 53.00\n",
            "IQR 범위 (Boxplot): -34.50 ~ 105.50\n",
            "--------------------------------------------------\n",
            "\n",
            "--- [2] 물리적 노이즈(10도 미만, 70도 초과) 제거 후 (110개) ---\n",
            "재계산된 Q1: 18.25 | Q3: 53.00\n",
            "재계산된 상/하한: -33.88 ~ 105.12\n",
            "--------------------------------------------------\n",
            "\n",
            "--- [3] 양쪽 끝 10% 절사(Trimming) 후 (99개) ---\n",
            "절사 기준: 12.00도 ~ 58.00도 사이만 남김\n",
            "재계산된 Q1: 18.00 | Q3: 51.00\n",
            "재계산된 상/하한: -31.50 ~ 100.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "FAY7YGX_zeOP",
        "outputId": "2955c029-169b-4db8-f7a5-fb0c67b76107",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 117 entries, 0 to 116\n",
            "Data columns (total 12 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   Dominant_Leg_Angle    117 non-null    float64\n",
            " 1   Dominant_Core_Angle   117 non-null    float64\n",
            " 2   Consistency_Score     117 non-null    float64\n",
            " 3   Leg_Stability_StdDev  117 non-null    float64\n",
            " 4   Leg_Fatigue_Slope     117 non-null    float64\n",
            " 5   Active_Frame_Count    117 non-null    float64\n",
            " 6   Active_Start_Frame    117 non-null    float64\n",
            " 7   Active_End_Frame      117 non-null    float64\n",
            " 8   Source_File           117 non-null    object \n",
            " 9   Person_ID             117 non-null    object \n",
            " 10  Level                 117 non-null    object \n",
            " 11  Place                 117 non-null    object \n",
            "dtypes: float64(8), object(4)\n",
            "memory usage: 11.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v4ecmqvvzfoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}